"""
åˆ‡åˆ†å™¨ - åªä¿ç•™å®Œæ•´çš„å±•ä½æ ‡æ³¨ï¼Œé¿å…å½¢çŠ¶è¢«åˆ‡å‰²
å¤„ç†åŸå§‹å›¾ç‰‡å’Œjsonæ–‡ä»¶, ç”Ÿæˆæ•°æ®é›†
"""
from pathlib import Path
from typing import Dict, Any, List, Tuple, Union
import shutil
import json
import random
import cv2
from shapely.geometry import Polygon, box
import shapely.affinity as affinity
from pypinyin import lazy_pinyin

# å¯¼å…¥å·¥ç¨‹åŒ–å·¥å…·
from src.utils import (
    get_logger,
    safe_mkdir,
    read_json,
    write_json,
    get_project_root,
    ensure_absolute,
)
from src.utils.image_tile_utils import TileCalculator
from src.core import DATASET_CONSTANTS

logger = get_logger('dataset_tiler')


def _convert_to_pinyin(name: str) -> str:
    """å°†ä¸­æ–‡åè½¬æ¢ä¸ºæ‹¼éŸ³"""
    return "".join(lazy_pinyin(name))

class Tiler:
    """
    YOLOæ•°æ®é›†åˆ‡åˆ†å™¨

    å…³é”®ä¿®å¤:
    1. åªä¿ç•™å®Œæ•´åœ¨åˆ‡ç‰‡å†…çš„å±•ä½æ ‡æ³¨ï¼ˆä¸åˆ‡å‰²å¤šè¾¹å½¢ï¼‰
    2. å¢å¤§overlapç¡®ä¿æ¯ä¸ªå±•ä½è‡³å°‘åœ¨ä¸€ä¸ªåˆ‡ç‰‡ä¸­æ˜¯å®Œæ•´çš„
    3. å¯é€‰ï¼šå¯¹äºéƒ¨åˆ†åœ¨åˆ‡ç‰‡å†…çš„å±•ä½ï¼Œä½¿ç”¨å…¶å®Œæ•´è¾¹ç•Œæ¡†
    """

    def __init__(self, config: Dict[str, Any]):
        self.image_path = Path(config["image_path"])
        self.json_path = Path(config["json_path"])
        self.output_dir = Path(config["output_dir"])
        self.tile_size = config.get("tile_size", DATASET_CONSTANTS.DEFAULT_TILE_SIZE)
        self.overlap = config.get("overlap", DATASET_CONSTANTS.DEFAULT_OVERLAP)
        self.class_names = config.get("class_names", ["booth"])
        self.min_area_ratio = config.get("min_area_ratio", DATASET_CONSTANTS.DEFAULT_MIN_AREA_RATIO)
        self.keep_only_complete = config.get("keep_only_complete", True)
        self.save_json = config.get("save_json", False)

        safe_mkdir(self.output_dir)

        self.img = cv2.imread(str(self.image_path))
        if self.img is None:
            raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {self.image_path}")

        self.labelme_data = read_json(self.json_path)

        logger.info(f"ğŸ–¼ï¸  åŸå›¾å°ºå¯¸: {self.img.shape[1]}x{self.img.shape[0]}")
        logger.info(f"ğŸ·ï¸  æ ‡æ³¨å¯¹è±¡æ•°é‡: {len(self.labelme_data['shapes'])}")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")

    def _get_all_tiles(self) -> List[Tuple[int, int, int, int, int]]:
        """è·å–æ‰€æœ‰åˆ‡ç‰‡ä½ç½®"""
        h, w = self.img.shape[:2]
        # ä½¿ç”¨ç»Ÿä¸€çš„åˆ‡å›¾è®¡ç®—å·¥å…·
        return TileCalculator.calculate_tiles(
            image_size=(h, w),
            tile_size=self.tile_size,
            overlap=self.overlap
        )

    def _is_polygon_complete_in_tile(self, poly: Polygon, tile_box: box) -> bool:
        """æ£€æŸ¥å¤šè¾¹å½¢æ˜¯å¦å®Œæ•´åœ¨åˆ‡ç‰‡å†…"""
        if not poly.intersects(tile_box):
            return False
        
        # è®¡ç®—äº¤é›†é¢ç§¯æ¯”ä¾‹
        intersection = poly.intersection(tile_box)
        area_ratio = intersection.area / poly.area if poly.area > 0 else 0
        
        return area_ratio >= self.min_area_ratio

    def _convert_annotation_fixed(self, shape: dict, x_offset: int, y_offset: int,
                                   tile_w: int, tile_h: int) -> Union[dict, None]:
        """
        ä¿®å¤ç‰ˆæ ‡æ³¨è½¬æ¢

        å…³é”®æ”¹å˜ï¼šä¸å†åˆ‡å‰²å¤šè¾¹å½¢ï¼Œåªä¿ç•™å®Œæ•´çš„å±•ä½
        """
        points = shape["points"]
        poly = Polygon(points)

        # æ£€æŸ¥å¤šè¾¹å½¢æœ‰æ•ˆæ€§
        if not poly.is_valid:
            return None

        # åˆ›å»ºåˆ‡ç‰‡è¾¹ç•Œæ¡†
        tile_box = box(x_offset, y_offset, x_offset + tile_w, y_offset + tile_h)

        # æ£€æŸ¥å¤šè¾¹å½¢æ˜¯å¦å®Œæ•´åœ¨åˆ‡ç‰‡å†…
        if not self._is_polygon_complete_in_tile(poly, tile_box):
            return None

        # è·å–åŸå§‹shape_type
        original_shape_type = shape.get("shape_type", "polygon")

        # æ ¹æ®é…ç½®å†³å®šå¤„ç†æ–¹å¼
        if self.keep_only_complete:
            result = self._process_complete_polygon(points, x_offset, y_offset, tile_w, tile_h)
        else:
            result = self._process_intersected_polygon(poly, tile_box, x_offset, y_offset, tile_w, tile_h)

        # ä¿ç•™åŸå§‹shape_type
        if result:
            result["shape_type"] = original_shape_type

        return result
    
    def _process_complete_polygon(self, points: List[List[float]], x_offset: int, y_offset: int, 
                                  tile_w: int, tile_h: int) -> Union[dict, None]:
        """å¤„ç†å®Œæ•´å¤šè¾¹å½¢"""
        # ç›´æ¥ä½¿ç”¨åŸå§‹å¤šè¾¹å½¢çš„ç‚¹ï¼Œä¸è¿›è¡Œåˆ‡å‰²ã€‚ åªæœ‰å½“å¤šè¾¹å½¢å‡ ä¹å®Œå…¨åœ¨åˆ‡ç‰‡å†…æ—¶æ‰ä½¿ç”¨åŸå§‹ç‚¹
        local_points = []
        for px, py in points:
            local_x = (px - x_offset) / tile_w
            local_y = (py - y_offset) / tile_h
            # è£å‰ªåˆ°[0, 1]èŒƒå›´
            local_x = max(0.0, min(1.0, local_x))
            local_y = max(0.0, min(1.0, local_y))
            local_points.append((local_x, local_y))
        
        # éªŒè¯ç‚¹æ•°ï¼ˆå±•ä½åº”è¯¥æ˜¯4ç‚¹å››è¾¹å½¢ï¼‰
        if len(local_points) != 4:
            logger.warning(f"    âš ï¸ è·³è¿‡éå››è¾¹å½¢æ ‡æ³¨ (ç‚¹æ•°: {len(local_points)})")
            return None
            
        return {
            "class_id": 0,
            "points": local_points,
            "original_points": len(points)
        }
    
    def _process_intersected_polygon(self, poly: Polygon, tile_box: box, 
                                     x_offset: int, y_offset: int, 
                                     tile_w: int, tile_h: int) -> Union[dict, None]:
        """å¤„ç†ç›¸äº¤å¤šè¾¹å½¢"""
        # å…è®¸ä½¿ç”¨äº¤é›†ï¼ˆä½†ä¼šæ”¹å˜å½¢çŠ¶ï¼‰
        intersection = poly.intersection(tile_box)
        if intersection.is_empty or intersection.area < 100:
            return None
            
        local_poly = affinity.translate(intersection, xoff=-x_offset, yoff=-y_offset)
        
        try:
            coords = list(local_poly.exterior.coords)[:-1]
        except:
            return None
            
        normalized_points = [(px / tile_w, py / tile_h) for px, py in coords]
        
        return {
            "class_id": 0,
            "points": normalized_points,
            "original_points": len(list(poly.exterior.coords)[:-1])  # åŸå§‹ç‚¹æ•°
        }

    def process(self) -> dict:
        """æ‰§è¡Œåˆ‡åˆ†"""
        all_tiles = self._get_all_tiles()
        logger.info(f"ğŸ” æ€»è®¡ {len(all_tiles)} ä¸ªåˆ‡ç‰‡ä½ç½®")

        stats = {'total': 0, 'kept': 0, 'skipped': 0}

        for tile_id, x, y, x_end, y_end in all_tiles:
            self._process_tile(tile_id, x, y, x_end, y_end, stats)

        logger.info(f"ğŸ“Š åˆ‡åˆ†å®Œæˆ: {len(all_tiles)} åˆ‡ç‰‡, ä¿ç•™ {stats['kept']} ä¸ªæ ‡æ³¨")

        return {
            'output_dir': str(self.output_dir),
            'total_tiles': len(all_tiles),
            'kept': stats['kept'],
            'skipped': stats['skipped']
        }

    def _process_tile(self, tile_id: int, x: int, y: int, x_end: int, y_end: int, stats: dict) -> dict:
        """å¤„ç†å•ä¸ªåˆ‡ç‰‡ - æ­¥éª¤1åªç”Ÿæˆpng+jsonï¼Œä¸åšåˆ†ç±»"""
        tile_w, tile_h = x_end - x, y_end - y
        tile_img = self.img[y:y_end, x:x_end]

        tile_name = f"{_convert_to_pinyin(self.image_path.stem)}_tile_{tile_id:04d}.png"

        # ä¿å­˜å›¾åƒ
        img_path = self.output_dir / tile_name
        cv2.imwrite(str(img_path), tile_img)

        # å¤„ç†æ ‡æ³¨
        annotations = []
        for shape in self.labelme_data["shapes"]:
            if shape["shape_type"] not in ("polygon", "rotation"):
                continue
            stats['total'] += 1
            ann = self._convert_annotation_fixed(shape, x, y, tile_w, tile_h)
            if ann:
                annotations.append(ann)
                stats['kept'] += 1
            else:
                stats['skipped'] += 1

        # æ­¥éª¤1ï¼šä¿å­˜JSONæ ¼å¼æ ‡æ³¨ï¼ˆæ­¥éª¤2å†æ ¹æ®æ˜¯å¦æœ‰æ ‡æ³¨è¿›è¡Œåˆ†ç±»ï¼‰
        if self.save_json:
            self._save_json_annotation(tile_name, tile_w, tile_h, annotations)

        return {'name': tile_name, 'annotations': len(annotations)}

    def _save_json_annotation(self, tile_name: str, tile_w: int, tile_h: int, annotations: List[dict]):
        """ä¿å­˜JSONæ ¼å¼æ ‡æ³¨ï¼ˆç”¨äºæ ‡æ³¨å·¥å…·æ£€æŸ¥ï¼‰"""

        # è½¬æ¢ä¸ºåƒç´ åæ ‡
        shapes = []
        for ann in annotations:
            points = [[px * tile_w, py * tile_h] for px, py in ann["points"]]
            shape_type = ann.get("shape_type", "polygon")  # ä»annotationä¸­è·å–shape_type
            shapes.append({
                "label": self.class_names[ann["class_id"]],
                "points": points,
                "group_id": None,
                "shape_type": shape_type,
                "flags": {}
            })

        # ç”ŸæˆJSONæ•°æ®
        json_data = {
            "version": "5.0.1",
            "flags": {},
            "shapes": shapes,
            "imagePath": tile_name,
            "imageData": None,
            "imageHeight": tile_h,
            "imageWidth": tile_w
        }

        # ä¿å­˜JSONæ–‡ä»¶ï¼ˆåŒçº§ç›®å½•ï¼‰
        write_json(self.output_dir / tile_name.replace(".png", ".json"), json_data, indent=2)


def find_matching_image(base_name: str, image_dir: Path) -> Path:
    """æ ¹æ®JSONæ–‡ä»¶åæŸ¥æ‰¾åŒ¹é…çš„å›¾ç‰‡æ–‡ä»¶"""
    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']
    
    # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
    for ext in image_extensions:
        image_path = image_dir / f"{base_name}{ext}"
        if image_path.exists():
            return image_path
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•åœ¨ç›®å½•ä¸­æŸ¥æ‰¾ç›¸ä¼¼åç§°çš„æ–‡ä»¶
    for img_file in image_dir.iterdir():
        if img_file.is_file() and img_file.stem == base_name and img_file.suffix.lower() in image_extensions:
            return img_file

    raise FileNotFoundError(f"æ‰¾ä¸åˆ°ä¸ {base_name} å¯¹åº”çš„å›¾ç‰‡æ–‡ä»¶")


# å·²åˆ é™¤ process_json_file å‡½æ•°ï¼Œä½¿ç”¨ process_dataset() ç»Ÿä¸€å¤„ç†

def process_dataset(
    input_source: str,
    image_dir: str = "images",
    output_base_dir: str = "datasets",
    final_output_dir: str = None,
    temp_dir: str = None,
    clean_temp: bool = True,
    tile_size: int = 640,
    overlap: int = 200,
    split_ratio: float = 0.8,
    min_area_ratio: float = 0.85,
    merge_manual_datasets: bool = False,
    manual_datasets_dir: str = "datasets",
    max_background_ratio: float = 0.3,  # èƒŒæ™¯å›¾åœ¨è®­ç»ƒé›†ä¸­çš„æœ€å¤§æ¯”ä¾‹
) -> dict:
    """
    é€šç”¨çš„æ•°æ®é›†å¤„ç†å‡½æ•°

    è¾“å…¥è§„åˆ™ï¼š
    - å•ä¸ªJSONæ–‡ä»¶: input_source="annotations/çº¢æœ¨.json" â†’ å¤„ç†å•ä¸ªæ–‡ä»¶
    - æ–‡ä»¶å¤¹: input_source="annotations" â†’ æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰JSON
    - é€—å·åˆ†éš”: input_source="file1.json,file2.json" â†’ æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶

    Args:
        input_source: è¾“å…¥æºï¼ˆJSONæ–‡ä»¶/æ–‡ä»¶å¤¹/é€—å·åˆ†éš”åˆ—è¡¨ï¼‰
        image_dir: å›¾ç‰‡ç›®å½•ï¼ˆé»˜è®¤: imagesï¼‰
        output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•ï¼ˆé»˜è®¤: datasetsï¼‰
        final_output_dir: æœ€ç»ˆåˆå¹¶è¾“å‡ºç›®å½•ï¼ˆä»…åœ¨ merge_manual_datasets=True æ—¶ä½¿ç”¨ï¼‰
        temp_dir: ä¸´æ—¶è¾“å‡ºç›®å½•ï¼ˆä»…åœ¨ merge_manual_datasets=True æ—¶ä½¿ç”¨ï¼‰
        clean_temp: æ˜¯å¦æ¸…ç†ä¸´æ—¶ç›®å½•ï¼ˆä»…åœ¨ merge_manual_datasets=True æ—¶ä½¿ç”¨ï¼‰
        tile_size: åˆ‡ç‰‡å¤§å°
        overlap: é‡å åŒºåŸŸå¤§å°
        split_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        min_area_ratio: æœ€å°ä¿ç•™æ¯”ä¾‹
        merge_manual_datasets: æ˜¯å¦åˆå¹¶æ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†ï¼ˆæ‰¹é‡æ¨¡å¼æ—¶ï¼‰
        max_background_ratio: èƒŒæ™¯å›¾åœ¨è®­ç»ƒé›†ä¸­çš„æœ€å¤§æ¯”ä¾‹ï¼ˆé»˜è®¤0.3=30%ï¼‰ï¼Œé¿å…èƒŒæ™¯å›¾è¿‡å¤š

    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼Œç”¨äºå°†ç›¸å¯¹è·¯å¾„è½¬ä¸ºç»å¯¹è·¯å¾„
    project_root = get_project_root()

    input_path = ensure_absolute(input_source, project_root)
    image_dir = ensure_absolute(image_dir, project_root)
    temp_dir = ensure_absolute(temp_dir, project_root) if temp_dir else project_root / "datasets" / "temp_tiler_output"

    # æ”¶é›†éœ€è¦å¤„ç†çš„JSONæ–‡ä»¶
    json_files = []

    # å¤„ç†å•ä¸ªJSONæ–‡ä»¶
    if input_path.is_file() and input_path.suffix.lower() == '.json':
        logger.info(f"ğŸ“ å¤„ç†å•ä¸ªJSONæ–‡ä»¶: {input_path}")
        json_files = [input_path]

    # å¤„ç†æ–‡ä»¶å¤¹
    elif input_path.is_dir():
        logger.info(f"ğŸ“‚ å¤„ç†æ–‡ä»¶å¤¹: {input_path}")
        json_files = list(input_path.glob('*.json'))
        if not json_files:
            logger.warning(f"âš ï¸  åœ¨ {input_path} ä¸­æœªæ‰¾åˆ°JSONæ–‡ä»¶")
            return {"error": "æœªæ‰¾åˆ°JSONæ–‡ä»¶"}
        logger.info(f"ğŸ” æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")

    # å¤„ç†å¤šä¸ªJSONæ–‡ä»¶åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰
    elif ',' in input_source:
        logger.info("ğŸ“š å¤„ç†å¤šä¸ªJSONæ–‡ä»¶åˆ—è¡¨")
        for path_str in input_source.split(','):
            json_file = Path(path_str.strip())
            if json_file.is_file():
                json_files.append(json_file)
                logger.info(f"  ğŸ“„ {json_file.name}")
            else:
                logger.error(f"  âŒ æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")

        if not json_files:
            logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„JSONæ–‡ä»¶")
            return {"error": "æ²¡æœ‰æœ‰æ•ˆçš„JSONæ–‡ä»¶"}
    else:
        logger.error(f"âŒ è¾“å…¥è·¯å¾„æ— æ•ˆ: {input_path}")
        logger.error("ğŸ’¡ è¯·æä¾›æœ‰æ•ˆçš„JSONæ–‡ä»¶è·¯å¾„ã€æ–‡ä»¶å¤¹è·¯å¾„æˆ–é€—å·åˆ†éš”çš„å¤šä¸ªæ–‡ä»¶è·¯å¾„")
        return {"error": "è¾“å…¥è·¯å¾„æ— æ•ˆ"}

    # ========== æ­¥éª¤1: åˆ‡åˆ† input_source ä¸­çš„ JSON æ–‡ä»¶ï¼ˆç”Ÿæˆåˆ‡ç‰‡å›¾ç‰‡ + JSONï¼‰ ==========
    # åˆ‡åˆ†åçš„æ•°æ®æ”¾åˆ° datasets/tmp/tiling_xx ä¸‹
    tmp_base_dir = project_root / "datasets" / "tmp"
    safe_mkdir(tmp_base_dir)

    results = {'processed': 0, 'failed': 0, 'total_tiles': 0, 'kept': 0}

    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“‹ æ­¥éª¤1: åˆ‡åˆ† JSON æ–‡ä»¶")
    logger.info("=" * 60)

    for json_file in sorted(json_files):
        json_stem = json_file.stem
        logger.info(f"\nğŸ“„ å¤„ç†: {json_stem}")

        try:
            # æŸ¥æ‰¾åŒ¹é…çš„å›¾ç‰‡
            image_path = find_matching_image(json_stem, image_dir)

            # åˆ‡åˆ†è¾“å‡ºç›®å½•ï¼šdatasets/tmp/tiling_xxï¼ˆxxä¸ºæ‹¼éŸ³åï¼‰
            pinyin_name = _convert_to_pinyin(json_stem)
            output_dir = tmp_base_dir / f"tiling_{pinyin_name}"
            safe_mkdir(output_dir)

            config = {
                "image_path": str(image_path),
                "json_path": str(json_file),
                "output_dir": str(output_dir),
                "tile_size": tile_size,
                "overlap": overlap,
                "split_ratio": split_ratio,
                "min_val_tiles": 3,
                "class_names": ["booth"],
                "dataset_name": json_stem,
                "min_area_ratio": min_area_ratio,
                "keep_only_complete": True,
                "save_json": True,  # ä¿å­˜åˆ‡ç‰‡åçš„ JSON
            }

            # åˆ›å»ºåˆ‡åˆ†å™¨å¹¶æ‰§è¡Œ
            tiler = Tiler(config)
            result = tiler.process()

            # ç´¯è®¡ç»Ÿè®¡
            results['processed'] += 1
            results['total_tiles'] += result['total_tiles']
            results['kept'] += result['kept']

            logger.info(f"âœ… {json_stem} å®Œæˆ: {result['total_tiles']} åˆ‡ç‰‡")

        except FileNotFoundError as e:
            logger.error(f"âŒ {json_stem} è·³è¿‡: {e}")
            results['failed'] += 1
            continue
        except Exception as e:
            logger.error(f"âŒ {json_stem} å¤±è´¥: {e}")
            results['failed'] += 1
            continue

    # ç»Ÿè®¡åˆ‡åˆ†ç»“æœ
    tiling_dirs = list(tmp_base_dir.glob('tiling_*'))
    total_png = sum(len(list(d.glob('*.png'))) for d in tiling_dirs)
    logger.info(f"\nğŸ“¦ æ­¥éª¤1å®Œæˆ: {len(tiling_dirs)} ä¸ªç›®å½•, {total_png} ä¸ªåˆ‡ç‰‡")

    # ========== æ­¥éª¤2: åˆå¹¶å¹¶åˆ†ç±»ï¼ˆannotated/backgroundï¼‰ ==========
    if merge_manual_datasets:
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“‹ æ­¥éª¤2: åˆå¹¶å¹¶åˆ†ç±»æ•°æ®")
        logger.info("=" * 60)

        # åˆ›å»º mix_tiling ç›®å½•ç”¨äºåˆ†ç±»å­˜å‚¨
        mix_dir = tmp_base_dir / "mix_tiling"
        mix_annotated_dir = mix_dir / "annotated"
        mix_background_dir = mix_dir / "background"
        safe_mkdir(mix_annotated_dir)
        safe_mkdir(mix_background_dir)

        # 1. å¤„ç† tiling_* ç›®å½•ä¸­çš„åˆ‡åˆ†æ•°æ®ï¼ˆæ ¹æ®JSONå†…å®¹åˆ†ç±»ï¼‰
        logger.info("ğŸ“‚ åˆ†ç±»åˆ‡åˆ†æ•°æ®...")
        for tiling_dir in tiling_dirs:
            for json_file in tiling_dir.glob('*.json'):
                try:
                    data = read_json(json_file)
                    has_annotation = len(data.get('shapes', [])) > 0
                    json_stem = json_file.stem
                    
                    # æŸ¥æ‰¾åŒ¹é…å›¾ç‰‡
                    img_file = None
                    for ext in ['.png', '.jpg', '.jpeg']:
                        candidate = json_file.parent / f"{json_stem}{ext}"
                        if candidate.exists():
                            img_file = candidate
                            break
                    
                    if not img_file:
                        logger.warning(f"âš ï¸  è·³è¿‡ {json_stem}: æ‰¾ä¸åˆ°åŒ¹é…å›¾ç‰‡")
                        continue
                    
                    # æ ¹æ®æ˜¯å¦æœ‰æ ‡æ³¨é€‰æ‹©ç›®æ ‡ç›®å½•
                    target_dir = mix_annotated_dir if has_annotation else mix_background_dir
                    shutil.copy2(json_file, target_dir)
                    shutil.copy2(img_file, target_dir)
                    
                except Exception as e:
                    logger.error(f"âŒ åˆ†ç±»å¤±è´¥ {json_file.name}: {e}")
                    continue

        # 2. åˆå¹¶ manual_datasets_dir ä¸­çš„æ‰‹åŠ¨æ ‡æ³¨æ•°æ®ï¼ˆå…¨éƒ¨è§†ä¸ºæœ‰æ ‡æ³¨ï¼‰
        if manual_datasets_dir:
            manual_dir = ensure_absolute(manual_datasets_dir, project_root)
            if manual_dir.is_dir():
                logger.info(f"ğŸ“‚ åˆå¹¶æ‰‹åŠ¨æ ‡æ³¨æ•°æ®: {manual_dir}")
                for json_file in manual_dir.glob('*.json'):
                    shutil.copy2(json_file, mix_annotated_dir)
                    json_stem = json_file.stem
                    for ext in ['.png', '.jpg', '.jpeg']:
                        img_file = manual_dir / f"{json_stem}{ext}"
                        if img_file.exists():
                            shutil.copy2(img_file, mix_annotated_dir)
                            break

        # ç»Ÿè®¡åˆ†ç±»ç»“æœ
        annotated_count = len(list(mix_annotated_dir.glob('*.json')))
        background_count = len(list(mix_background_dir.glob('*.json')))
        logger.info(f"\nğŸ“¦ æ­¥éª¤2å®Œæˆ:")
        logger.info(f"   âœ… æœ‰æ ‡æ³¨(annotated): {annotated_count} ä¸ª")
        logger.info(f"   âšª èƒŒæ™¯å›¾(background): {background_count} ä¸ª")

        # ========== æ­¥éª¤3: å°† tmp ç›®å½•è½¬æ¢ä¸º YOLO æ ¼å¼æ•°æ®é›† ==========
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“‹ æ­¥éª¤3: è½¬æ¢ä¸º YOLO æ ¼å¼æ•°æ®é›†")
        logger.info("=" * 60)

        final_output_dir = ensure_absolute(final_output_dir, project_root) if final_output_dir else project_root / "datasets" / "booth_final_merged"
        final_train_img_dir = final_output_dir / "images" / "train"
        final_train_lbl_dir = final_output_dir / "labels" / "train"
        final_val_img_dir = final_output_dir / "images" / "val"
        final_val_lbl_dir = final_output_dir / "labels" / "val"

        # æ¸…ç†å·²å­˜åœ¨çš„è¾“å‡ºç›®å½•ï¼ˆé¿å…é‡å¤æ‰§è¡Œæ—¶å›¾ç‰‡ç´¯ç§¯ï¼‰
        if final_output_dir.exists():
            logger.info(f"ğŸ§¹ æ¸…ç†å·²å­˜åœ¨çš„è¾“å‡ºç›®å½•: {final_output_dir}")
            shutil.rmtree(final_output_dir)

        for dir_path in [final_train_img_dir, final_train_lbl_dir, final_val_img_dir, final_val_lbl_dir]:
            safe_mkdir(dir_path)

        # å¤„ç† mix_tiling ç›®å½•ä¸­çš„æ‰€æœ‰ JSONï¼ˆä»åˆ†ç±»åçš„ç›®å½•è¯»å–ï¼‰
        json_count = 0
        train_count = 0
        val_count = 0
        
        # å…ˆå¤„ç† annotated ç›®å½•ï¼ˆæœ‰æ ‡æ³¨çš„æŒ‰ split_ratio åˆ†é…ï¼‰
        for json_file in mix_annotated_dir.glob('*.json'):
            try:
                json_stem = json_file.stem
                json_dir = json_file.parent
                # æŸ¥æ‰¾åŒ¹é…å›¾ç‰‡ï¼ˆåœ¨åŒç›®å½•ï¼‰
                image_file = None
                for ext in ['.png', '.jpg', '.jpeg']:
                    candidate = json_dir / f"{json_stem}{ext}"
                    if candidate.exists():
                        image_file = candidate
                        break

                if not image_file:
                    logger.warning(f"âš ï¸  è·³è¿‡ {json_stem}: æ‰¾ä¸åˆ°åŒ¹é…å›¾ç‰‡")
                    continue

                # è¯»å– JSON å¹¶è½¬æ¢ä¸º YOLO æ ¼å¼
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                img_width = data.get('imageWidth', data.get('image_width', 0))
                img_height = data.get('imageHeight', data.get('image_height', 0))

                if img_width == 0 or img_height == 0:
                    # å°è¯•ä»å›¾ç‰‡è·å–å°ºå¯¸
                    from PIL import Image
                    with Image.open(image_file) as img:
                        img_width, img_height = img.size

                yolo_annotations = []
                for shape in data.get('shapes', []):
                    label = shape.get('label', 'booth')
                    points = shape.get('points', [])
                    if len(points) >= 4:
                        # å°†å¤šè¾¹å½¢è½¬æ¢ä¸ºå½’ä¸€åŒ–åæ ‡
                        x_coords = [p[0] / img_width for p in points]
                        y_coords = [p[1] / img_height for p in points]
                        yolo_ann = '0 ' + ' '.join([f"{x:.6f} {y:.6f}" for x, y in zip(x_coords, y_coords)])
                        yolo_annotations.append(yolo_ann)

                # å‡†å¤‡æ ‡æ³¨å†…å®¹
                label_content = '\n'.join(yolo_annotations) + '\n' if yolo_annotations else ''

                # annotated ç›®å½•ï¼šæœ‰æ ‡æ³¨çš„æŒ‰ split_ratio åˆ†é…
                is_train = random.random() < split_ratio

                try:
                    if is_train:
                        shutil.copy2(image_file, final_train_img_dir / image_file.name)
                        (final_train_lbl_dir / f"{json_stem}.txt").write_text(label_content, encoding='utf-8')
                        train_count += 1
                    else:
                        shutil.copy2(image_file, final_val_img_dir / image_file.name)
                        (final_val_lbl_dir / f"{json_stem}.txt").write_text(label_content, encoding='utf-8')
                        val_count += 1
                    json_count += 1
                except Exception as copy_err:
                    logger.error(f"âŒ å¤åˆ¶å¤±è´¥ {json_stem}: {copy_err}")
                    continue

            except Exception as e:
                logger.error(f"âŒ è½¬æ¢å¤±è´¥ {json_file.name}: {e}")
                continue

        # å¤„ç† background ç›®å½•ï¼ˆæŒ‰ max_background_ratio é™åˆ¶æ•°é‡ï¼‰
        logger.info("ğŸ“‚ å¤„ç†èƒŒæ™¯å›¾...")
        
        # è®¡ç®—åº”è¯¥ä¿ç•™çš„èƒŒæ™¯å›¾æ•°é‡
        # å½“å‰ train_count æ˜¯æœ‰æ ‡æ³¨çš„å›¾ç‰‡æ•°é‡ï¼ˆannotated ä¸­åˆ†é…åˆ° train çš„ï¼‰
        annotated_train_count = train_count  # æ­¤æ—¶ train_count åªåŒ…å« annotated çš„è®­ç»ƒé›†å›¾ç‰‡
        max_background_count = int(annotated_train_count * max_background_ratio / (1 - max_background_ratio))
        
        # æ”¶é›†æ‰€æœ‰èƒŒæ™¯å›¾
        background_files = list(mix_background_dir.glob('*.json'))
        
        # éšæœºé‡‡æ ·ï¼Œé™åˆ¶èƒŒæ™¯å›¾æ•°é‡
        if len(background_files) > max_background_count:
            logger.info(f"   âš ï¸ èƒŒæ™¯å›¾è¿‡å¤š: {len(background_files)} ä¸ªï¼Œé™åˆ¶ä¸º {max_background_count} ä¸ª (æ¯”ä¾‹ {max_background_ratio:.0%})")
            random.shuffle(background_files)
            background_files = background_files[:max_background_count]
        else:
            logger.info(f"   âœ… èƒŒæ™¯å›¾æ•°é‡: {len(background_files)} ä¸ª (é™åˆ¶: {max_background_count} ä¸ª)")
        
        for json_file in background_files:
            try:
                json_stem = json_file.stem
                # æŸ¥æ‰¾åŒ¹é…å›¾ç‰‡
                image_file = None
                for ext in ['.png', '.jpg', '.jpeg']:
                    candidate = mix_background_dir / f"{json_stem}{ext}"
                    if candidate.exists():
                        image_file = candidate
                        break

                if not image_file:
                    logger.warning(f"âš ï¸  è·³è¿‡ {json_stem}: æ‰¾ä¸åˆ°åŒ¹é…å›¾ç‰‡")
                    continue

                # èƒŒæ™¯å›¾ï¼šç©ºæ ‡æ³¨ï¼Œå¼ºåˆ¶æ”¾è®­ç»ƒé›†
                shutil.copy2(image_file, final_train_img_dir / image_file.name)
                (final_train_lbl_dir / f"{json_stem}.txt").write_text('', encoding='utf-8')
                train_count += 1
                json_count += 1

            except Exception as e:
                logger.error(f"âŒ å¤„ç†èƒŒæ™¯å›¾å¤±è´¥ {json_file.name}: {e}")
                continue

        logger.info(f"\nğŸ“¦ æ­¥éª¤3å®Œæˆ: è½¬æ¢ {json_count} ä¸ª JSON åˆ° YOLO æ ¼å¼")

        # ç”Ÿæˆ dataset.yaml
        path_str = str(final_output_dir.absolute())
        yaml_content = f"""# æœ€ç»ˆåˆå¹¶æ•°æ®é›†
path: {path_str}
train: images/train
val: images/val

names:
  0: booth
"""
        (final_output_dir / "dataset.yaml").write_text(yaml_content, encoding='utf-8')

        # ç»Ÿè®¡æœ‰å†…å®¹çš„æ ‡æ³¨æ–‡ä»¶æ•°ï¼ˆæ’é™¤ç©ºæ–‡ä»¶ï¼‰
        train_labels_with_content = sum(1 for f in final_train_lbl_dir.glob('*.txt') if f.stat().st_size > 0)
        val_labels_with_content = sum(1 for f in final_val_lbl_dir.glob('*.txt') if f.stat().st_size > 0)

        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š æœ€ç»ˆç»Ÿè®¡")
        logger.info("=" * 60)
        logger.info(f"è®­ç»ƒé›†: {len(list(final_train_img_dir.glob('*')))} å›¾ç‰‡, {len(list(final_train_lbl_dir.glob('*.txt')))} æ ‡æ³¨æ–‡ä»¶ ({train_labels_with_content} æœ‰å†…å®¹)")
        logger.info(f"éªŒè¯é›†: {len(list(final_val_img_dir.glob('*')))} å›¾ç‰‡, {len(list(final_val_lbl_dir.glob('*.txt')))} æ ‡æ³¨æ–‡ä»¶ ({val_labels_with_content} æœ‰å†…å®¹)")
        logger.info(f"è¾“å‡º: {final_output_dir}")
        logger.info("=" * 60)

        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if clean_temp and temp_dir.exists():
            logger.info(f"\nğŸ§¹ æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")
            try:
                shutil.rmtree(temp_dir)
                logger.info("âœ… ä¸´æ—¶ç›®å½•å·²åˆ é™¤")
            except Exception as e:
                logger.warning(f"âš ï¸  æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")

    # ========== æ‰“å°æœ€ç»ˆç»Ÿè®¡ ==========
    if not merge_manual_datasets:
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š å¤„ç†ç»Ÿè®¡")
        logger.info("=" * 60)
        logger.info(f"å¤„ç†æ–‡ä»¶: {results['processed']}, å¤±è´¥: {results['failed']}")
        logger.info(f"æ€»åˆ‡ç‰‡: {results['total_tiles']}, ä¿ç•™æ ‡æ³¨: {results['kept']}")
        logger.info("=" * 60)

    return results


if __name__ == "__main__":
    # # æ¨¡å¼1: å¤„ç†å•ä¸ªæ–‡ä»¶
    # process_dataset("annotations/çº¢æœ¨.json")

    # æ¨¡å¼2: æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹
    # process_dataset("annotations/çº¢æœ¨.json,annotations/11å±ŠçŒªä¸š.json")

    # æ¨¡å¼3: æ‰¹é‡å¤„ç† + åˆå¹¶æ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†
    process_dataset(
        input_source="annotations/",
        merge_manual_datasets=True,
        manual_datasets_dir="datasets/manual_booth_annotations",
        final_output_dir="datasets/booth_final_merged",
        clean_temp=True,
        tile_size=640,
        overlap=200,
        max_background_ratio=0.3,  # èƒŒæ™¯å›¾æœ€å¤šå è®­ç»ƒé›†çš„30%
    )