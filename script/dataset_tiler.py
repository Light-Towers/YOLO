"""
åˆ‡åˆ†å™¨ - åªä¿ç•™å®Œæ•´çš„å±•ä½æ ‡æ³¨ï¼Œé¿å…å½¢çŠ¶è¢«åˆ‡å‰²
å¤„ç†åŸå§‹å›¾ç‰‡å’Œjsonæ–‡ä»¶, ç”Ÿæˆæ•°æ®é›†
"""
import json
from pathlib import Path
from typing import Dict, Any, List, Tuple, Union
import cv2
import numpy as np
from shapely.geometry import Polygon, box
import shapely.affinity as affinity
from pypinyin import lazy_pinyin
from log_config import get_project_logger

logger = get_project_logger('dataset_tiler')

class Tiler:
    """
    YOLOæ•°æ®é›†åˆ‡åˆ†å™¨

    å…³é”®ä¿®å¤:
    1. åªä¿ç•™å®Œæ•´åœ¨åˆ‡ç‰‡å†…çš„å±•ä½æ ‡æ³¨ï¼ˆä¸åˆ‡å‰²å¤šè¾¹å½¢ï¼‰
    2. å¢å¤§overlapç¡®ä¿æ¯ä¸ªå±•ä½è‡³å°‘åœ¨ä¸€ä¸ªåˆ‡ç‰‡ä¸­æ˜¯å®Œæ•´çš„
    3. å¯é€‰ï¼šå¯¹äºéƒ¨åˆ†åœ¨åˆ‡ç‰‡å†…çš„å±•ä½ï¼Œä½¿ç”¨å…¶å®Œæ•´è¾¹ç•Œæ¡†
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.image_path = Path(config["image_path"])
        self.json_path = Path(config["json_path"])
        # å¯¹è¾“å‡ºç›®å½•åè¿›è¡Œä¸­æ–‡è½¬æ‹¼éŸ³å¤„ç†
        original_output_dir = config["output_dir"]
        self.output_dir_name = self._convert_chinese_to_pinyin(Path(original_output_dir).name)
        self.output_dir = Path(original_output_dir).parent / self.output_dir_name
        self.tile_size = config.get("tile_size", 640)
        self.overlap = config.get("overlap", 200)  # å¢å¤§é»˜è®¤overlap
        self.split_ratio = config.get("split_ratio", 0.8)
        self.min_val_tiles = config.get("min_val_tiles", 2)
        self.class_names = config.get("class_names", ["booth"])
        self.dataset_name = self._convert_chinese_to_pinyin(config.get("dataset_name", "fixed_dataset"))
        
        # æ–°å¢é…ç½®ï¼šæœ€å°ä¿ç•™æ¯”ä¾‹ï¼ˆå±•ä½é¢ç§¯åœ¨åˆ‡ç‰‡å†…çš„æ¯”ä¾‹ï¼‰
        self.min_area_ratio = config.get("min_area_ratio", 0.9)  # 90%ä»¥ä¸Šæ‰ä¿ç•™
        # æ–°å¢é…ç½®ï¼šæ˜¯å¦åªä¿ç•™å®Œæ•´çš„4ç‚¹å¤šè¾¹å½¢
        self.keep_only_complete = config.get("keep_only_complete", True)
        # æ–°å¢é…ç½®ï¼šæ˜¯å¦ä¿å­˜JSONæ ¼å¼æ ‡æ³¨
        self.save_json = config.get("save_json", False)

        self._create_output_structure()
        
        self.img = cv2.imread(str(self.image_path))
        if self.img is None:
            raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {self.image_path}")

        with open(self.json_path, 'r', encoding='utf-8') as f:
            self.labelme_data = json.load(f)

        logger.info(f"ğŸ–¼ï¸  åŸå›¾å°ºå¯¸: {self.img.shape[1]}x{self.img.shape[0]}")
        logger.info(f"ğŸ·ï¸  æ ‡æ³¨å¯¹è±¡æ•°é‡: {len(self.labelme_data['shapes'])}")
        logger.info(f"ğŸ“Š åˆ‡ç‰‡å‚æ•°: size={self.tile_size}, overlap={self.overlap}")
        logger.info(f"âš™ï¸  åªä¿ç•™å®Œæ•´æ ‡æ³¨: {self.keep_only_complete}")
        logger.info(f"âš™ï¸  æœ€å°é¢ç§¯æ¯”ä¾‹: {self.min_area_ratio:.0%}")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")

    def _create_output_structure(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
        for split in ['train', 'val']:
            (self.output_dir / "images" / split).mkdir(parents=True, exist_ok=True)
            (self.output_dir / "labels" / split).mkdir(parents=True, exist_ok=True)
            if self.save_json:
                (self.output_dir / "json_annotations" / split).mkdir(parents=True, exist_ok=True)

        yaml_content = self._generate_yaml_content()
        (self.output_dir / "dataset.yaml").write_text(yaml_content, encoding='utf-8')
        logger.info(f"âœ… å·²åˆ›å»ºæ•°æ®é›†ç»“æ„: {self.output_dir}")

    def _generate_yaml_content(self) -> str:
        path_str = str(self.output_dir.absolute())
        names_block = "names:\n"
        for i, name in enumerate(self.class_names):
            names_block += f"  {i}: {name}\n"

        return f"""# {self.dataset_name} - ä¿®å¤ç‰ˆYOLOæ•°æ®é›†é…ç½®
path: {path_str}
train: images/train
val: images/val

# ç±»åˆ«
{names_block}
"""

    def _get_all_tiles(self) -> List[Tuple[int, int, int, int, int]]:
        """è·å–æ‰€æœ‰åˆ‡ç‰‡ä½ç½®"""
        h, w = self.img.shape[:2]
        tiles = []
        tile_id = 0
        step = self.tile_size - self.overlap

        y = 0
        while y < h:
            x = 0
            while x < w:
                x_end = min(x + self.tile_size, w)
                y_end = min(y + self.tile_size, h)
                tiles.append((tile_id, x, y, x_end, y_end))
                tile_id += 1
                x += step
            y += step

        return tiles

    def _assign_splits(self, tiles: list) -> Dict[str, list]:
        """åˆ†é…è®­ç»ƒ/éªŒè¯é›†"""
        total = len(tiles)
        val_count = max(self.min_val_tiles, int(total * (1 - self.split_ratio)))
        
        if total <= 2:
            val_count = 1
        
        train_tiles = tiles[:-val_count] if val_count < total else tiles[:1]
        val_tiles = tiles[-val_count:] if val_count > 0 else [tiles[-1]]

        logger.info(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ†: è®­ç»ƒé›† {len(train_tiles)}, éªŒè¯é›† {len(val_tiles)}")
        return {'train': train_tiles, 'val': val_tiles}

    def _is_polygon_complete_in_tile(self, poly: Polygon, tile_box: box) -> bool:
        """æ£€æŸ¥å¤šè¾¹å½¢æ˜¯å¦å®Œæ•´åœ¨åˆ‡ç‰‡å†…"""
        if not poly.intersects(tile_box):
            return False
        
        # è®¡ç®—äº¤é›†é¢ç§¯æ¯”ä¾‹
        intersection = poly.intersection(tile_box)
        area_ratio = intersection.area / poly.area if poly.area > 0 else 0
        
        return area_ratio >= self.min_area_ratio

    # æ·»åŠ ä¸­æ–‡è½¬æ‹¼éŸ³æ–¹æ³•
    def _convert_chinese_to_pinyin(self, text):
        """å°†ä¸­æ–‡è½¬æ¢ä¸ºæ‹¼éŸ³"""
        if not text:
            return text

        try:
            pinyin_list = lazy_pinyin(text)
            result = ''.join(pinyin_list).lower()
            logger.info(f"ğŸ”¤ '{text}' -> '{result}'")
            return result
        except:
            return text

    def _convert_annotation_fixed(self, shape: dict, x_offset: int, y_offset: int,
                                   tile_w: int, tile_h: int) -> Union[dict, None]:
        """
        ä¿®å¤ç‰ˆæ ‡æ³¨è½¬æ¢

        å…³é”®æ”¹å˜ï¼šä¸å†åˆ‡å‰²å¤šè¾¹å½¢ï¼Œåªä¿ç•™å®Œæ•´çš„å±•ä½
        """
        points = shape["points"]
        poly = Polygon(points)

        # æ£€æŸ¥å¤šè¾¹å½¢æœ‰æ•ˆæ€§
        if not poly.is_valid:
            return None

        # åˆ›å»ºåˆ‡ç‰‡è¾¹ç•Œæ¡†
        tile_box = box(x_offset, y_offset, x_offset + tile_w, y_offset + tile_h)

        # æ£€æŸ¥å¤šè¾¹å½¢æ˜¯å¦å®Œæ•´åœ¨åˆ‡ç‰‡å†…
        if not self._is_polygon_complete_in_tile(poly, tile_box):
            return None

        # è·å–åŸå§‹shape_type
        original_shape_type = shape.get("shape_type", "polygon")

        # æ ¹æ®é…ç½®å†³å®šå¤„ç†æ–¹å¼
        if self.keep_only_complete:
            result = self._process_complete_polygon(points, x_offset, y_offset, tile_w, tile_h)
        else:
            result = self._process_intersected_polygon(poly, tile_box, x_offset, y_offset, tile_w, tile_h)

        # ä¿ç•™åŸå§‹shape_type
        if result:
            result["shape_type"] = original_shape_type

        return result
    
    def _process_complete_polygon(self, points: List[List[float]], x_offset: int, y_offset: int, 
                                  tile_w: int, tile_h: int) -> Union[dict, None]:
        """å¤„ç†å®Œæ•´å¤šè¾¹å½¢"""
        # ç›´æ¥ä½¿ç”¨åŸå§‹å¤šè¾¹å½¢çš„ç‚¹ï¼Œä¸è¿›è¡Œåˆ‡å‰²ã€‚ åªæœ‰å½“å¤šè¾¹å½¢å‡ ä¹å®Œå…¨åœ¨åˆ‡ç‰‡å†…æ—¶æ‰ä½¿ç”¨åŸå§‹ç‚¹
        local_points = []
        for px, py in points:
            local_x = (px - x_offset) / tile_w
            local_y = (py - y_offset) / tile_h
            # è£å‰ªåˆ°[0, 1]èŒƒå›´
            local_x = max(0.0, min(1.0, local_x))
            local_y = max(0.0, min(1.0, local_y))
            local_points.append((local_x, local_y))
        
        # éªŒè¯ç‚¹æ•°ï¼ˆå±•ä½åº”è¯¥æ˜¯4ç‚¹å››è¾¹å½¢ï¼‰
        if len(local_points) != 4:
            logger.warning(f"    âš ï¸ è·³è¿‡éå››è¾¹å½¢æ ‡æ³¨ (ç‚¹æ•°: {len(local_points)})")
            return None
            
        return {
            "class_id": 0,
            "points": local_points,
            "original_points": len(points)
        }
    
    def _process_intersected_polygon(self, poly: Polygon, tile_box: box, 
                                     x_offset: int, y_offset: int, 
                                     tile_w: int, tile_h: int) -> Union[dict, None]:
        """å¤„ç†ç›¸äº¤å¤šè¾¹å½¢"""
        # å…è®¸ä½¿ç”¨äº¤é›†ï¼ˆä½†ä¼šæ”¹å˜å½¢çŠ¶ï¼‰
        intersection = poly.intersection(tile_box)
        if intersection.is_empty or intersection.area < 100:
            return None
            
        local_poly = affinity.translate(intersection, xoff=-x_offset, yoff=-y_offset)
        
        try:
            coords = list(local_poly.exterior.coords)[:-1]
        except:
            return None
            
        normalized_points = [(px / tile_w, py / tile_h) for px, py in coords]
        
        return {
            "class_id": 0,
            "points": normalized_points,
            "original_points": len(list(poly.exterior.coords)[:-1])  # åŸå§‹ç‚¹æ•°
        }

    def process(self) -> dict:
        """æ‰§è¡Œåˆ‡åˆ†"""
        all_tiles = self._get_all_tiles()
        logger.info(f"ğŸ” æ€»è®¡ {len(all_tiles)} ä¸ªåˆ‡ç‰‡ä½ç½®")

        splits = self._assign_splits(all_tiles)
        results = {'train': [], 'val': []}
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_annotations': 0,
            'skipped_incomplete': 0,
            'kept_complete': 0
        }

        for split_name, tiles in splits.items():
            for tile_id, x, y, x_end, y_end in tiles:
                # å¤„ç†å•ä¸ªåˆ‡ç‰‡
                tile_result = self._process_tile(split_name, tile_id, x, y, x_end, y_end, stats)
                results[split_name].append(tile_result)

        # æ‰“å°ç»Ÿè®¡
        self._print_statistics(all_tiles, results, stats)

        return {
            'output_dir': str(self.output_dir),
            'yaml_path': str(self.output_dir / "dataset.yaml"),
            'train_tiles': len(results['train']),
            'val_tiles': len(results['val']),
            'kept_complete': stats['kept_complete'],
            'skipped_incomplete': stats['skipped_incomplete']
        }
    
    def _process_tile(self, split_name: str, tile_id: int, x: int, y: int, x_end: int, y_end: int, stats: dict) -> dict:
        """å¤„ç†å•ä¸ªåˆ‡ç‰‡"""
        tile_w, tile_h = x_end - x, y_end - y
        tile_img = self.img[y:y_end, x:x_end]
        
        # å¯¹åŸå§‹å›¾ç‰‡æ–‡ä»¶åè¿›è¡Œä¸­æ–‡è½¬æ‹¼éŸ³å¤„ç†
        original_stem = self.image_path.stem
        converted_stem = self._convert_chinese_to_pinyin(original_stem)
        tile_name = f"{converted_stem}_tile_{tile_id:04d}.png"

        # ä¿å­˜å›¾åƒ
        img_path = self.output_dir / "images" / split_name / tile_name
        cv2.imwrite(str(img_path), tile_img)

        # å¤„ç†æ ‡æ³¨
        annotations = []
        for shape in self.labelme_data["shapes"]:
            # åªå¤„ç†å¤šè¾¹å½¢å’Œæ—‹è½¬æ¡†
            if shape["shape_type"] != "polygon" and shape["shape_type"] != "rotation":
                continue

            stats['total_annotations'] += 1
            ann = self._convert_annotation_fixed(shape, x, y, tile_w, tile_h)
            
            if ann:
                annotations.append(ann)
                stats['kept_complete'] += 1
            else:
                stats['skipped_incomplete'] += 1

        # ä¿å­˜æ ‡æ³¨
        lbl_path = self.output_dir / "labels" / split_name / tile_name.replace(".png", ".txt")
        with open(lbl_path, 'w') as f:
            for ann in annotations:
                points_str = " ".join([f"{px:.6f} {py:.6f}" for px, py in ann["points"]])
                f.write(f"0 {points_str}\n")

        # ä¿å­˜JSONæ ¼å¼æ ‡æ³¨ï¼ˆç”¨äºæ ‡æ³¨å·¥å…·æ£€æŸ¥ï¼‰
        if self.save_json:
            self._save_json_annotation(split_name, tile_name, tile_w, tile_h, annotations)

        status = "âœ…" if annotations else "ğŸŸ¡"
        logger.info(f"{status} {split_name}: {tile_name} - {len(annotations)} ä¸ªå®Œæ•´å±•ä½")

        return {
            'name': tile_name,
            'annotations': len(annotations),
            'position': (x, y, x_end, y_end)
        }

    def _save_json_annotation(self, split_name: str, tile_name: str, tile_w: int, tile_h: int, annotations: List[dict]):
        """ä¿å­˜JSONæ ¼å¼æ ‡æ³¨ï¼ˆç”¨äºæ ‡æ³¨å·¥å…·æ£€æŸ¥ï¼‰"""
        # è½¬æ¢ä¸ºåƒç´ åæ ‡
        shapes = []
        for ann in annotations:
            points = [[px * tile_w, py * tile_h] for px, py in ann["points"]]
            shape_type = ann.get("shape_type", "polygon")  # ä»annotationä¸­è·å–shape_type
            shapes.append({
                "label": self.class_names[ann["class_id"]],
                "points": points,
                "group_id": None,
                "shape_type": shape_type,
                "flags": {}
            })

        # ç”ŸæˆJSONæ•°æ®
        json_data = {
            "version": "5.0.1",
            "flags": {},
            "shapes": shapes,
            "imagePath": tile_name,
            "imageData": None,
            "imageHeight": tile_h,
            "imageWidth": tile_w
        }

        # ä¿å­˜JSONæ–‡ä»¶
        json_path = self.output_dir / "json_annotations" / split_name / tile_name.replace(".png", ".json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)

    def _print_statistics(self, all_tiles: list, results: dict, stats: dict):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š åˆ‡åˆ†ç»Ÿè®¡æŠ¥å‘Š")
        logger.info("=" * 60)
        logger.info(f"æ€»åˆ‡ç‰‡æ•°: {len(all_tiles)}")
        logger.info(f"è®­ç»ƒé›†æ ‡æ³¨: {sum(t['annotations'] for t in results['train'])}")
        logger.info(f"éªŒè¯é›†æ ‡æ³¨: {sum(t['annotations'] for t in results['val'])}")
        logger.info(f"ä¿ç•™çš„å®Œæ•´å±•ä½: {stats['kept_complete']}")
        logger.info(f"è·³è¿‡çš„ä¸å®Œæ•´å±•ä½: {stats['skipped_incomplete']}")
        logger.info(f"ä¿ç•™ç‡: {stats['kept_complete'] / max(stats['total_annotations'], 1):.1%}")
        logger.info("=" * 60)


def find_matching_image(base_name: str, image_dir: Path) -> Path:
    """æ ¹æ®JSONæ–‡ä»¶åæŸ¥æ‰¾åŒ¹é…çš„å›¾ç‰‡æ–‡ä»¶"""
    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']
    
    # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
    for ext in image_extensions:
        image_path = image_dir / f"{base_name}{ext}"
        if image_path.exists():
            return image_path
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•åœ¨ç›®å½•ä¸­æŸ¥æ‰¾ç›¸ä¼¼åç§°çš„æ–‡ä»¶
    for img_file in image_dir.iterdir():
        if img_file.is_file() and img_file.stem == base_name and img_file.suffix.lower() in image_extensions:
            return img_file

    raise FileNotFoundError(f"æ‰¾ä¸åˆ°ä¸ {base_name} å¯¹åº”çš„å›¾ç‰‡æ–‡ä»¶")


def process_json_file(json_path: Path, image_dir: Path = Path("images")):
    """å¤„ç†å•ä¸ªJSONæ–‡ä»¶"""
    # è·å–JSONæ–‡ä»¶çš„åŸºç¡€åç§°ï¼ˆä¸å«æ‰©å±•åï¼‰
    json_stem = json_path.stem
    image_path = find_matching_image(json_stem, image_dir)

    config = {
        "image_path": str(image_path),
        "json_path": str(json_path),
        "output_dir": f"datasets/{json_stem}",  # æ ¹æ®JSONæ–‡ä»¶åç”Ÿæˆè¾“å‡ºç›®å½•å
        
        # åˆ‡ç‰‡å‚æ•° - å…³é”®ä¿®æ”¹
        "tile_size": 640,
        "overlap": 200,  # å¢å¤§overlapï¼Œç¡®ä¿æ›´å¤šå±•ä½åœ¨æŸä¸ªåˆ‡ç‰‡ä¸­æ˜¯å®Œæ•´çš„        
        # æ•°æ®é›†å‚æ•°
        "split_ratio": 0.8,
        "min_val_tiles": 3,
        "class_names": ["booth"],  # ä½¿ç”¨æ›´æœ‰æ„ä¹‰çš„ç±»å
        "dataset_name": json_stem,
        "min_area_ratio": 0.85,  # å±•ä½85%ä»¥ä¸Šåœ¨åˆ‡ç‰‡å†…æ‰ä¿ç•™
        "keep_only_complete": True,  # åªä¿ç•™å®Œæ•´çš„4ç‚¹å››è¾¹å½¢
        "save_json": False,  # æ˜¯å¦ä¿å­˜JSONæ ¼å¼æ ‡æ³¨ï¼ˆé»˜è®¤å…³é—­ï¼‰
    }

    logger.info(f"ğŸ”§ ä½¿ç”¨é…ç½®: {json_stem}")
    logger.info(f"ğŸ“„ JSONæ–‡ä»¶: {json_path.name}")
    logger.info(f"ğŸ–¼ï¸  åŒ¹é…å›¾ç‰‡: {image_path.name}")

    # åˆ›å»ºåˆ‡åˆ†å™¨å¹¶æ‰§è¡Œ
    tiler = Tiler(config)
    result = tiler.process()

    logger.info(f"\nâœ… æ•°æ®é›†å·²ç”Ÿæˆ: {result['output_dir']}")
    logger.info(f"ğŸ“„ YAMLé…ç½®: {result['yaml_path']}")

def main(input_source: str = r"labelme_annotations/11-ZhuYe.json"):
    """ä¸»å‡½æ•° - ç”¨äºåˆ‡åˆ†"""
    image_dir = Path("images")
    input_path = Path(input_source)

    # å¤„ç†å•ä¸ªJSONæ–‡ä»¶
    if input_path.is_file() and input_path.suffix.lower() == '.json':
        logger.info(f"ğŸ“ å¤„ç†å•ä¸ªJSONæ–‡ä»¶: {input_path}")
        process_json_file(input_path, image_dir)
    # å¤„ç†æ–‡ä»¶å¤¹
    elif input_path.is_dir():
        logger.info(f"ğŸ“‚ å¤„ç†æ–‡ä»¶å¤¹: {input_path}")
        json_files = list(input_path.glob('*.json'))
        if not json_files:
            logger.warning(f"âš ï¸  åœ¨ {input_path} ä¸­æœªæ‰¾åˆ°JSONæ–‡ä»¶")
        else:
            logger.info(f"ğŸ” æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
            for json_file in json_files:
                logger.info(f"  ğŸ“„ {json_file.name}")
                process_json_file(json_file, image_dir)
    # å¤„ç†å¤šä¸ªJSONæ–‡ä»¶åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰
    elif ',' in input_source:
        logger.info("ğŸ“š å¤„ç†å¤šä¸ªJSONæ–‡ä»¶åˆ—è¡¨")
        for path_str in input_source.split(','):
            json_file = Path(path_str.strip())
            if json_file.is_file():
                logger.info(f"  ğŸ“„ {json_file.name}")
                process_json_file(json_file, image_dir)
            else:
                logger.error(f"  âŒ æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
    else:
        logger.error(f"âŒ è¾“å…¥è·¯å¾„æ— æ•ˆ: {input_path}")
        logger.error("ğŸ’¡ è¯·æä¾›æœ‰æ•ˆçš„JSONæ–‡ä»¶è·¯å¾„ã€æ–‡ä»¶å¤¹è·¯å¾„æˆ–é€—å·åˆ†éš”çš„å¤šä¸ªæ–‡ä»¶è·¯å¾„")


if __name__ == "__main__":
    # å¯ä»¥åœ¨è¿™é‡ŒæŒ‡å®šinput_sourceå‚æ•°ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤å€¼
    input_source = r"labelme_annotations/æµ‹è¯•åˆ‡å›¾_222.json"
    main(input_source)