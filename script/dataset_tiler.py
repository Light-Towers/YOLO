"""
åˆ‡åˆ†å™¨ - åªä¿ç•™å®Œæ•´çš„å±•ä½æ ‡æ³¨ï¼Œé¿å…å½¢çŠ¶è¢«åˆ‡å‰²
å¤„ç†åŸå§‹å›¾ç‰‡å’Œjsonæ–‡ä»¶, ç”Ÿæˆæ•°æ®é›†
"""
from pathlib import Path
from typing import Dict, Any, List, Tuple, Union
import shutil
import cv2
from shapely.geometry import Polygon, box
import shapely.affinity as affinity
from pypinyin import lazy_pinyin

# å¯¼å…¥å·¥ç¨‹åŒ–å·¥å…·
from src.utils import (
    get_logger,
    safe_mkdir,
    read_json,
    write_json,
)
from src.utils.image_tile_utils import TileCalculator
from src.core import DATASET_CONSTANTS

logger = get_logger('dataset_tiler')

class Tiler:
    """
    YOLOæ•°æ®é›†åˆ‡åˆ†å™¨

    å…³é”®ä¿®å¤:
    1. åªä¿ç•™å®Œæ•´åœ¨åˆ‡ç‰‡å†…çš„å±•ä½æ ‡æ³¨ï¼ˆä¸åˆ‡å‰²å¤šè¾¹å½¢ï¼‰
    2. å¢å¤§overlapç¡®ä¿æ¯ä¸ªå±•ä½è‡³å°‘åœ¨ä¸€ä¸ªåˆ‡ç‰‡ä¸­æ˜¯å®Œæ•´çš„
    3. å¯é€‰ï¼šå¯¹äºéƒ¨åˆ†åœ¨åˆ‡ç‰‡å†…çš„å±•ä½ï¼Œä½¿ç”¨å…¶å®Œæ•´è¾¹ç•Œæ¡†
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.image_path = Path(config["image_path"])
        self.json_path = Path(config["json_path"])
        # å¯¹è¾“å‡ºç›®å½•åè¿›è¡Œä¸­æ–‡è½¬æ‹¼éŸ³å¤„ç†
        original_output_dir = config["output_dir"]
        self.output_dir_name = self._convert_chinese_to_pinyin(Path(original_output_dir).name)
        self.output_dir = Path(original_output_dir).parent / self.output_dir_name
        self.tile_size = config.get("tile_size", DATASET_CONSTANTS.DEFAULT_TILE_SIZE)
        self.overlap = config.get("overlap", DATASET_CONSTANTS.DEFAULT_OVERLAP)
        self.split_ratio = config.get("split_ratio", DATASET_CONSTANTS.DEFAULT_TRAIN_RATIO)
        self.min_val_tiles = config.get("min_val_tiles", DATASET_CONSTANTS.DEFAULT_MIN_VAL_TILES)
        self.class_names = config.get("class_names", ["booth"])
        self.dataset_name = self._convert_chinese_to_pinyin(config.get("dataset_name", "fixed_dataset"))

        # æ–°å¢é…ç½®ï¼šæœ€å°ä¿ç•™æ¯”ä¾‹ï¼ˆå±•ä½é¢ç§¯åœ¨åˆ‡ç‰‡å†…çš„æ¯”ä¾‹ï¼‰
        self.min_area_ratio = config.get("min_area_ratio", DATASET_CONSTANTS.DEFAULT_MIN_AREA_RATIO)
        # æ–°å¢é…ç½®ï¼šæ˜¯å¦åªä¿ç•™å®Œæ•´çš„4ç‚¹å¤šè¾¹å½¢
        self.keep_only_complete = config.get("keep_only_complete", True)
        # æ–°å¢é…ç½®ï¼šæ˜¯å¦ä¿å­˜JSONæ ¼å¼æ ‡æ³¨
        self.save_json = config.get("save_json", False)

        self._create_output_structure()

        self.img = cv2.imread(str(self.image_path))
        if self.img is None:
            raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {self.image_path}")

        # ä½¿ç”¨å·¥å…·å‡½æ•°è¯»å–JSON
        self.labelme_data = read_json(self.json_path)

        logger.info(f"ğŸ–¼ï¸  åŸå›¾å°ºå¯¸: {self.img.shape[1]}x{self.img.shape[0]}")
        logger.info(f"ğŸ·ï¸  æ ‡æ³¨å¯¹è±¡æ•°é‡: {len(self.labelme_data['shapes'])}")
        logger.info(f"ğŸ“Š åˆ‡ç‰‡å‚æ•°: size={self.tile_size}, overlap={self.overlap}")
        logger.info(f"âš™ï¸  åªä¿ç•™å®Œæ•´æ ‡æ³¨: {self.keep_only_complete}")
        logger.info(f"âš™ï¸  æœ€å°é¢ç§¯æ¯”ä¾‹: {self.min_area_ratio:.0%}")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")

    def _create_output_structure(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
        for split in ['train', 'val']:
            safe_mkdir(self.output_dir / "images" / split)
            safe_mkdir(self.output_dir / "labels" / split)
            if self.save_json:
                safe_mkdir(self.output_dir / "json_annotations" / split)

        yaml_content = self._generate_yaml_content()
        (self.output_dir / "dataset.yaml").write_text(yaml_content, encoding='utf-8')
        logger.info(f"âœ… å·²åˆ›å»ºæ•°æ®é›†ç»“æ„: {self.output_dir}")

    def _generate_yaml_content(self) -> str:
        path_str = str(self.output_dir.absolute())
        names_block = "names:\n"
        for i, name in enumerate(self.class_names):
            names_block += f"  {i}: {name}\n"

        return f"""# {self.dataset_name} - ä¿®å¤ç‰ˆYOLOæ•°æ®é›†é…ç½®
path: {path_str}
train: images/train
val: images/val

# ç±»åˆ«
{names_block}
"""

    def _get_all_tiles(self) -> List[Tuple[int, int, int, int, int]]:
        """è·å–æ‰€æœ‰åˆ‡ç‰‡ä½ç½®"""
        h, w = self.img.shape[:2]
        # ä½¿ç”¨ç»Ÿä¸€çš„åˆ‡å›¾è®¡ç®—å·¥å…·
        return TileCalculator.calculate_tiles(
            image_size=(h, w),
            tile_size=self.tile_size,
            overlap=self.overlap
        )

    def _assign_splits(self, tiles: list) -> Dict[str, list]:
        """åˆ†é…è®­ç»ƒ/éªŒè¯é›†"""
        total = len(tiles)
        val_count = max(self.min_val_tiles, int(total * (1 - self.split_ratio)))
        
        if total <= 2:
            val_count = 1
        
        train_tiles = tiles[:-val_count] if val_count < total else tiles[:1]
        val_tiles = tiles[-val_count:] if val_count > 0 else [tiles[-1]]

        logger.info(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ†: è®­ç»ƒé›† {len(train_tiles)}, éªŒè¯é›† {len(val_tiles)}")
        return {'train': train_tiles, 'val': val_tiles}

    def _is_polygon_complete_in_tile(self, poly: Polygon, tile_box: box) -> bool:
        """æ£€æŸ¥å¤šè¾¹å½¢æ˜¯å¦å®Œæ•´åœ¨åˆ‡ç‰‡å†…"""
        if not poly.intersects(tile_box):
            return False
        
        # è®¡ç®—äº¤é›†é¢ç§¯æ¯”ä¾‹
        intersection = poly.intersection(tile_box)
        area_ratio = intersection.area / poly.area if poly.area > 0 else 0
        
        return area_ratio >= self.min_area_ratio

    # æ·»åŠ ä¸­æ–‡è½¬æ‹¼éŸ³æ–¹æ³•
    def _convert_chinese_to_pinyin(self, text):
        """å°†ä¸­æ–‡è½¬æ¢ä¸ºæ‹¼éŸ³"""
        if not text:
            return text

        try:
            pinyin_list = lazy_pinyin(text)
            result = ''.join(pinyin_list).lower()
            # logger.info(f"ğŸ”¤ '{text}' -> '{result}'")
            return result
        except:
            return text

    def _convert_annotation_fixed(self, shape: dict, x_offset: int, y_offset: int,
                                   tile_w: int, tile_h: int) -> Union[dict, None]:
        """
        ä¿®å¤ç‰ˆæ ‡æ³¨è½¬æ¢

        å…³é”®æ”¹å˜ï¼šä¸å†åˆ‡å‰²å¤šè¾¹å½¢ï¼Œåªä¿ç•™å®Œæ•´çš„å±•ä½
        """
        points = shape["points"]
        poly = Polygon(points)

        # æ£€æŸ¥å¤šè¾¹å½¢æœ‰æ•ˆæ€§
        if not poly.is_valid:
            return None

        # åˆ›å»ºåˆ‡ç‰‡è¾¹ç•Œæ¡†
        tile_box = box(x_offset, y_offset, x_offset + tile_w, y_offset + tile_h)

        # æ£€æŸ¥å¤šè¾¹å½¢æ˜¯å¦å®Œæ•´åœ¨åˆ‡ç‰‡å†…
        if not self._is_polygon_complete_in_tile(poly, tile_box):
            return None

        # è·å–åŸå§‹shape_type
        original_shape_type = shape.get("shape_type", "polygon")

        # æ ¹æ®é…ç½®å†³å®šå¤„ç†æ–¹å¼
        if self.keep_only_complete:
            result = self._process_complete_polygon(points, x_offset, y_offset, tile_w, tile_h)
        else:
            result = self._process_intersected_polygon(poly, tile_box, x_offset, y_offset, tile_w, tile_h)

        # ä¿ç•™åŸå§‹shape_type
        if result:
            result["shape_type"] = original_shape_type

        return result
    
    def _process_complete_polygon(self, points: List[List[float]], x_offset: int, y_offset: int, 
                                  tile_w: int, tile_h: int) -> Union[dict, None]:
        """å¤„ç†å®Œæ•´å¤šè¾¹å½¢"""
        # ç›´æ¥ä½¿ç”¨åŸå§‹å¤šè¾¹å½¢çš„ç‚¹ï¼Œä¸è¿›è¡Œåˆ‡å‰²ã€‚ åªæœ‰å½“å¤šè¾¹å½¢å‡ ä¹å®Œå…¨åœ¨åˆ‡ç‰‡å†…æ—¶æ‰ä½¿ç”¨åŸå§‹ç‚¹
        local_points = []
        for px, py in points:
            local_x = (px - x_offset) / tile_w
            local_y = (py - y_offset) / tile_h
            # è£å‰ªåˆ°[0, 1]èŒƒå›´
            local_x = max(0.0, min(1.0, local_x))
            local_y = max(0.0, min(1.0, local_y))
            local_points.append((local_x, local_y))
        
        # éªŒè¯ç‚¹æ•°ï¼ˆå±•ä½åº”è¯¥æ˜¯4ç‚¹å››è¾¹å½¢ï¼‰
        if len(local_points) != 4:
            logger.warning(f"    âš ï¸ è·³è¿‡éå››è¾¹å½¢æ ‡æ³¨ (ç‚¹æ•°: {len(local_points)})")
            return None
            
        return {
            "class_id": 0,
            "points": local_points,
            "original_points": len(points)
        }
    
    def _process_intersected_polygon(self, poly: Polygon, tile_box: box, 
                                     x_offset: int, y_offset: int, 
                                     tile_w: int, tile_h: int) -> Union[dict, None]:
        """å¤„ç†ç›¸äº¤å¤šè¾¹å½¢"""
        # å…è®¸ä½¿ç”¨äº¤é›†ï¼ˆä½†ä¼šæ”¹å˜å½¢çŠ¶ï¼‰
        intersection = poly.intersection(tile_box)
        if intersection.is_empty or intersection.area < 100:
            return None
            
        local_poly = affinity.translate(intersection, xoff=-x_offset, yoff=-y_offset)
        
        try:
            coords = list(local_poly.exterior.coords)[:-1]
        except:
            return None
            
        normalized_points = [(px / tile_w, py / tile_h) for px, py in coords]
        
        return {
            "class_id": 0,
            "points": normalized_points,
            "original_points": len(list(poly.exterior.coords)[:-1])  # åŸå§‹ç‚¹æ•°
        }

    def process(self) -> dict:
        """æ‰§è¡Œåˆ‡åˆ†"""
        all_tiles = self._get_all_tiles()
        logger.info(f"ğŸ” æ€»è®¡ {len(all_tiles)} ä¸ªåˆ‡ç‰‡ä½ç½®")

        splits = self._assign_splits(all_tiles)
        results = {'train': [], 'val': []}
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_annotations': 0,
            'skipped_incomplete': 0,
            'kept_complete': 0
        }

        for split_name, tiles in splits.items():
            for tile_id, x, y, x_end, y_end in tiles:
                # å¤„ç†å•ä¸ªåˆ‡ç‰‡
                tile_result = self._process_tile(split_name, tile_id, x, y, x_end, y_end, stats)
                results[split_name].append(tile_result)

        # æ‰“å°ç»Ÿè®¡
        self._print_statistics(all_tiles, results, stats)

        return {
            'output_dir': str(self.output_dir),
            'yaml_path': str(self.output_dir / "dataset.yaml"),
            'train_tiles': len(results['train']),
            'val_tiles': len(results['val']),
            'kept_complete': stats['kept_complete'],
            'skipped_incomplete': stats['skipped_incomplete']
        }
    
    def _process_tile(self, split_name: str, tile_id: int, x: int, y: int, x_end: int, y_end: int, stats: dict) -> dict:
        """å¤„ç†å•ä¸ªåˆ‡ç‰‡"""
        tile_w, tile_h = x_end - x, y_end - y
        tile_img = self.img[y:y_end, x:x_end]
        
        # å¯¹åŸå§‹å›¾ç‰‡æ–‡ä»¶åè¿›è¡Œä¸­æ–‡è½¬æ‹¼éŸ³å¤„ç†
        original_stem = self.image_path.stem
        converted_stem = self._convert_chinese_to_pinyin(original_stem)
        tile_name = f"{converted_stem}_tile_{tile_id:04d}.png"

        # ä¿å­˜å›¾åƒ
        img_path = self.output_dir / "images" / split_name / tile_name
        cv2.imwrite(str(img_path), tile_img)

        # å¤„ç†æ ‡æ³¨
        annotations = []
        for shape in self.labelme_data["shapes"]:
            # åªå¤„ç†å¤šè¾¹å½¢å’Œæ—‹è½¬æ¡†
            if shape["shape_type"] != "polygon" and shape["shape_type"] != "rotation":
                continue

            stats['total_annotations'] += 1
            ann = self._convert_annotation_fixed(shape, x, y, tile_w, tile_h)
            
            if ann:
                annotations.append(ann)
                stats['kept_complete'] += 1
            else:
                stats['skipped_incomplete'] += 1

        # ä¿å­˜æ ‡æ³¨
        lbl_path = self.output_dir / "labels" / split_name / tile_name.replace(".png", ".txt")
        with open(lbl_path, 'w') as f:
            for ann in annotations:
                points_str = " ".join([f"{px:.6f} {py:.6f}" for px, py in ann["points"]])
                f.write(f"0 {points_str}\n")

        # ä¿å­˜JSONæ ¼å¼æ ‡æ³¨ï¼ˆç”¨äºæ ‡æ³¨å·¥å…·æ£€æŸ¥ï¼‰
        if self.save_json:
            self._save_json_annotation(split_name, tile_name, tile_w, tile_h, annotations)

        status = "âœ…" if annotations else "ğŸŸ¡"
        logger.info(f"{status} {split_name}: {tile_name} - {len(annotations)} ä¸ªå®Œæ•´å±•ä½")

        return {
            'name': tile_name,
            'annotations': len(annotations),
            'position': (x, y, x_end, y_end)
        }

    def _save_json_annotation(self, split_name: str, tile_name: str, tile_w: int, tile_h: int, annotations: List[dict]):
        """ä¿å­˜JSONæ ¼å¼æ ‡æ³¨ï¼ˆç”¨äºæ ‡æ³¨å·¥å…·æ£€æŸ¥ï¼‰"""
        # è½¬æ¢ä¸ºåƒç´ åæ ‡
        shapes = []
        for ann in annotations:
            points = [[px * tile_w, py * tile_h] for px, py in ann["points"]]
            shape_type = ann.get("shape_type", "polygon")  # ä»annotationä¸­è·å–shape_type
            shapes.append({
                "label": self.class_names[ann["class_id"]],
                "points": points,
                "group_id": None,
                "shape_type": shape_type,
                "flags": {}
            })

        # ç”ŸæˆJSONæ•°æ®
        json_data = {
            "version": "5.0.1",
            "flags": {},
            "shapes": shapes,
            "imagePath": tile_name,
            "imageData": None,
            "imageHeight": tile_h,
            "imageWidth": tile_w
        }

        # ä¿å­˜JSONæ–‡ä»¶
        json_path = self.output_dir / "json_annotations" / split_name / tile_name.replace(".png", ".json")
        write_json(json_path, json_data, ensure_ascii=False, indent=2)

    def _print_statistics(self, all_tiles: list, results: dict, stats: dict):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š åˆ‡åˆ†ç»Ÿè®¡æŠ¥å‘Š")
        logger.info("=" * 60)
        logger.info(f"æ€»åˆ‡ç‰‡æ•°: {len(all_tiles)}")
        logger.info(f"è®­ç»ƒé›†æ ‡æ³¨: {sum(t['annotations'] for t in results['train'])}")
        logger.info(f"éªŒè¯é›†æ ‡æ³¨: {sum(t['annotations'] for t in results['val'])}")
        logger.info(f"ä¿ç•™çš„å®Œæ•´å±•ä½: {stats['kept_complete']}")
        logger.info(f"è·³è¿‡çš„ä¸å®Œæ•´å±•ä½: {stats['skipped_incomplete']}")
        logger.info(f"ä¿ç•™ç‡: {stats['kept_complete'] / max(stats['total_annotations'], 1):.1%}")
        logger.info("=" * 60)


def find_matching_image(base_name: str, image_dir: Path) -> Path:
    """æ ¹æ®JSONæ–‡ä»¶åæŸ¥æ‰¾åŒ¹é…çš„å›¾ç‰‡æ–‡ä»¶"""
    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']
    
    # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
    for ext in image_extensions:
        image_path = image_dir / f"{base_name}{ext}"
        if image_path.exists():
            return image_path
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•åœ¨ç›®å½•ä¸­æŸ¥æ‰¾ç›¸ä¼¼åç§°çš„æ–‡ä»¶
    for img_file in image_dir.iterdir():
        if img_file.is_file() and img_file.stem == base_name and img_file.suffix.lower() in image_extensions:
            return img_file

    raise FileNotFoundError(f"æ‰¾ä¸åˆ°ä¸ {base_name} å¯¹åº”çš„å›¾ç‰‡æ–‡ä»¶")


# å·²åˆ é™¤ process_json_file å‡½æ•°ï¼Œä½¿ç”¨ process_dataset() ç»Ÿä¸€å¤„ç†

def process_dataset(
    input_source: str,
    image_dir: str = "images",
    output_base_dir: str = "datasets",
    final_output_dir: str = None,
    temp_dir: str = None,
    clean_temp: bool = True,
    tile_size: int = 640,
    overlap: int = 200,
    split_ratio: float = 0.8,
    min_area_ratio: float = 0.85,
    merge_manual_datasets: bool = False,
    manual_datasets_dir: str = "datasets",
) -> dict:
    """
    é€šç”¨çš„æ•°æ®é›†å¤„ç†å‡½æ•°

    è¾“å…¥è§„åˆ™ï¼š
    - å•ä¸ªJSONæ–‡ä»¶: input_source="annotations/çº¢æœ¨.json" â†’ å¤„ç†å•ä¸ªæ–‡ä»¶
    - æ–‡ä»¶å¤¹: input_source="annotations" â†’ æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰JSON
    - é€—å·åˆ†éš”: input_source="file1.json,file2.json" â†’ æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶

    Args:
        input_source: è¾“å…¥æºï¼ˆJSONæ–‡ä»¶/æ–‡ä»¶å¤¹/é€—å·åˆ†éš”åˆ—è¡¨ï¼‰
        image_dir: å›¾ç‰‡ç›®å½•ï¼ˆé»˜è®¤: imagesï¼‰
        output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•ï¼ˆé»˜è®¤: datasetsï¼‰
        final_output_dir: æœ€ç»ˆåˆå¹¶è¾“å‡ºç›®å½•ï¼ˆä»…åœ¨ merge_manual_datasets=True æ—¶ä½¿ç”¨ï¼‰
        temp_dir: ä¸´æ—¶è¾“å‡ºç›®å½•ï¼ˆä»…åœ¨ merge_manual_datasets=True æ—¶ä½¿ç”¨ï¼‰
        clean_temp: æ˜¯å¦æ¸…ç†ä¸´æ—¶ç›®å½•ï¼ˆä»…åœ¨ merge_manual_datasets=True æ—¶ä½¿ç”¨ï¼‰
        tile_size: åˆ‡ç‰‡å¤§å°
        overlap: é‡å åŒºåŸŸå¤§å°
        split_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        min_area_ratio: æœ€å°ä¿ç•™æ¯”ä¾‹
        merge_manual_datasets: æ˜¯å¦åˆå¹¶æ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†ï¼ˆæ‰¹é‡æ¨¡å¼æ—¶ï¼‰

    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    input_path = Path(input_source)
    image_dir = Path(image_dir)
    temp_dir = Path(temp_dir) if temp_dir else Path("datasets/temp_tiler_output")

    # æ”¶é›†éœ€è¦å¤„ç†çš„JSONæ–‡ä»¶
    json_files = []

    # å¤„ç†å•ä¸ªJSONæ–‡ä»¶
    if input_path.is_file() and input_path.suffix.lower() == '.json':
        logger.info(f"ğŸ“ å¤„ç†å•ä¸ªJSONæ–‡ä»¶: {input_path}")
        json_files = [input_path]

    # å¤„ç†æ–‡ä»¶å¤¹
    elif input_path.is_dir():
        logger.info(f"ğŸ“‚ å¤„ç†æ–‡ä»¶å¤¹: {input_path}")
        json_files = list(input_path.glob('*.json'))
        if not json_files:
            logger.warning(f"âš ï¸  åœ¨ {input_path} ä¸­æœªæ‰¾åˆ°JSONæ–‡ä»¶")
            return {"error": "æœªæ‰¾åˆ°JSONæ–‡ä»¶"}
        logger.info(f"ğŸ” æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")

    # å¤„ç†å¤šä¸ªJSONæ–‡ä»¶åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰
    elif ',' in input_source:
        logger.info("ğŸ“š å¤„ç†å¤šä¸ªJSONæ–‡ä»¶åˆ—è¡¨")
        for path_str in input_source.split(','):
            json_file = Path(path_str.strip())
            if json_file.is_file():
                json_files.append(json_file)
                logger.info(f"  ğŸ“„ {json_file.name}")
            else:
                logger.error(f"  âŒ æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")

        if not json_files:
            logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„JSONæ–‡ä»¶")
            return {"error": "æ²¡æœ‰æœ‰æ•ˆçš„JSONæ–‡ä»¶"}
    else:
        logger.error(f"âŒ è¾“å…¥è·¯å¾„æ— æ•ˆ: {input_path}")
        logger.error("ğŸ’¡ è¯·æä¾›æœ‰æ•ˆçš„JSONæ–‡ä»¶è·¯å¾„ã€æ–‡ä»¶å¤¹è·¯å¾„æˆ–é€—å·åˆ†éš”çš„å¤šä¸ªæ–‡ä»¶è·¯å¾„")
        return {"error": "è¾“å…¥è·¯å¾„æ— æ•ˆ"}

    # ========== å¤„ç†JSONæ–‡ä»¶ ==========
    tilered_datasets = []
    results = {
        'processed': 0,
        'failed': 0,
        'train_tiles': 0,
        'val_tiles': 0,
        'kept_complete': 0,
        'skipped_incomplete': 0,
    }

    for json_file in sorted(json_files):
        json_stem = json_file.stem
        logger.info(f"\nğŸ“„ å¤„ç†: {json_stem}")

        try:
            # æŸ¥æ‰¾åŒ¹é…çš„å›¾ç‰‡
            image_path = find_matching_image(json_stem, image_dir)

            # æ„å»ºé…ç½®
            if merge_manual_datasets:
                # æ‰¹é‡+åˆå¹¶æ¨¡å¼ï¼šè¾“å‡ºåˆ°ä¸´æ—¶ç›®å½•
                output_dir = temp_dir / json_stem
            else:
                # å•ç‹¬/æ‰¹é‡æ¨¡å¼ï¼šè¾“å‡ºåˆ°ç‹¬ç«‹ç›®å½•
                output_dir = Path(output_base_dir) / json_stem

            config = {
                "image_path": str(image_path),
                "json_path": str(json_file),
                "output_dir": str(output_dir),
                "tile_size": tile_size,
                "overlap": overlap,
                "split_ratio": split_ratio,
                "min_val_tiles": 3,
                "class_names": ["booth"],
                "dataset_name": json_stem,
                "min_area_ratio": min_area_ratio,
                "keep_only_complete": True,
                "save_json": False,
            }

            # åˆ›å»ºåˆ‡åˆ†å™¨å¹¶æ‰§è¡Œ
            tiler = Tiler(config)
            result = tiler.process()

            tilered_datasets.append(Path(result['output_dir']))

            # ç´¯è®¡ç»Ÿè®¡
            results['processed'] += 1
            results['train_tiles'] += result['train_tiles']
            results['val_tiles'] += result['val_tiles']
            results['kept_complete'] += result['kept_complete']
            results['skipped_incomplete'] += result['skipped_incomplete']

            logger.info(f"âœ… {json_stem} å®Œæˆ: {result['train_tiles']} è®­ç»ƒåˆ‡ç‰‡, {result['val_tiles']} éªŒè¯åˆ‡ç‰‡")

        except FileNotFoundError as e:
            logger.error(f"âŒ {json_stem} è·³è¿‡: {e}")
            results['failed'] += 1
            continue
        except Exception as e:
            logger.error(f"âŒ {json_stem} å¤±è´¥: {e}")
            results['failed'] += 1
            continue

    # ========== åˆå¹¶æ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†ï¼ˆå¯é€‰ï¼‰ ==========
    if merge_manual_datasets:
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ”— åˆå¹¶æ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†")
        logger.info("=" * 60)

        final_output_dir = Path(final_output_dir) if final_output_dir else Path("datasets/booth_final_merged")
        safe_mkdir(temp_dir)

        # æ”¶é›†æ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†
        manual_datasets_dir = Path(manual_datasets_dir)
        valid_datasets = []

        for dataset_dir in manual_datasets_dir.iterdir():
            if not dataset_dir.is_dir():
                continue

            required_dirs = [
                dataset_dir / "images" / "train",
                dataset_dir / "images" / "val",
                dataset_dir / "labels" / "train",
                dataset_dir / "labels" / "val",
            ]

            if all(d.exists() for d in required_dirs):
                train_imgs = len(list((dataset_dir / "images" / "train").glob("*")))
                val_imgs = len(list((dataset_dir / "images" / "val").glob("*")))

                if train_imgs > 0 or val_imgs > 0:
                    valid_datasets.append(dataset_dir)
                    logger.info(f"âœ… {dataset_dir.name}: {train_imgs} è®­ç»ƒ, {val_imgs} éªŒè¯")

        # åˆå¹¶æ‰€æœ‰æ•°æ®é›†
        all_datasets = tilered_datasets + valid_datasets
        logger.info(f"ğŸ“¦ å¾…åˆå¹¶: {len(all_datasets)} ä¸ª")

        final_train_img_dir = final_output_dir / "images" / "train"
        final_train_lbl_dir = final_output_dir / "labels" / "train"
        final_val_img_dir = final_output_dir / "images" / "val"
        final_val_lbl_dir = final_output_dir / "labels" / "val"

        for dir_path in [final_train_img_dir, final_train_lbl_dir, final_val_img_dir, final_val_lbl_dir]:
            safe_mkdir(dir_path)

        merge_stats = {
            'train_images': 0,
            'val_images': 0,
            'train_annotations': 0,
            'val_annotations': 0,
            'datasets_count': 0,
        }

        for dataset_dir in all_datasets:
            logger.info(f"\nğŸ”— åˆå¹¶: {dataset_dir.name}")
            dataset_prefix = f"{dataset_dir.name}_"

            # è®­ç»ƒé›†
            train_img_dir = dataset_dir / "images" / "train"
            train_lbl_dir = dataset_dir / "labels" / "train"

            if train_img_dir.exists():
                for img_file in train_img_dir.glob("*"):
                    if img_file.is_file():
                        new_name = f"{dataset_prefix}{img_file.name}"
                        shutil.copy2(img_file, final_train_img_dir / new_name)

                        label_file = train_lbl_dir / img_file.with_suffix('.txt').name
                        if label_file.exists():
                            shutil.copy2(label_file, final_train_lbl_dir / new_name)
                            merge_stats['train_annotations'] += 1

                merge_stats['train_images'] += len(list(train_img_dir.glob("*")))

            # éªŒè¯é›†
            val_img_dir = dataset_dir / "images" / "val"
            val_lbl_dir = dataset_dir / "labels" / "val"

            if val_img_dir.exists():
                for img_file in val_img_dir.glob("*"):
                    if img_file.is_file():
                        new_name = f"{dataset_prefix}{img_file.name}"
                        shutil.copy2(img_file, final_val_img_dir / new_name)

                        label_file = val_lbl_dir / img_file.with_suffix('.txt').name
                        if label_file.exists():
                            shutil.copy2(label_file, final_val_lbl_dir / new_name)
                            merge_stats['val_annotations'] += 1

                merge_stats['val_images'] += len(list(val_img_dir.glob("*")))

            merge_stats['datasets_count'] += 1

        # ç”Ÿæˆ dataset.yaml
        path_str = str(final_output_dir.absolute())
        yaml_content = f"""# æœ€ç»ˆåˆå¹¶æ•°æ®é›†
path: {path_str}
train: images/train
val: images/val

names:
  0: booth
"""
        (final_output_dir / "dataset.yaml").write_text(yaml_content, encoding='utf-8')

        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š åˆå¹¶ç»Ÿè®¡")
        logger.info("=" * 60)
        logger.info(f"åˆå¹¶æ•°æ®é›†: {merge_stats['datasets_count']}")
        logger.info(f"è®­ç»ƒé›†å›¾ç‰‡: {merge_stats['train_images']}")
        logger.info(f"éªŒè¯é›†å›¾ç‰‡: {merge_stats['val_images']}")
        logger.info(f"è®­ç»ƒé›†æ ‡æ³¨: {merge_stats['train_annotations']}")
        logger.info(f"éªŒè¯é›†æ ‡æ³¨: {merge_stats['val_annotations']}")
        logger.info(f"è¾“å‡º: {final_output_dir}")
        logger.info("=" * 60)

        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if clean_temp and temp_dir.exists():
            logger.info(f"\nğŸ§¹ æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")
            try:
                shutil.rmtree(temp_dir)
                logger.info("âœ… ä¸´æ—¶ç›®å½•å·²åˆ é™¤")
            except Exception as e:
                logger.warning(f"âš ï¸  æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")

        # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
        results.update(merge_stats)

    # ========== æ‰“å°æœ€ç»ˆç»Ÿè®¡ ==========
    if not merge_manual_datasets:
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š å¤„ç†ç»Ÿè®¡")
        logger.info("=" * 60)
        logger.info(f"å¤„ç†æ–‡ä»¶: {results['processed']}")
        logger.info(f"å¤±è´¥æ–‡ä»¶: {results['failed']}")
        logger.info(f"è®­ç»ƒé›†åˆ‡ç‰‡: {results['train_tiles']}")
        logger.info(f"éªŒè¯é›†åˆ‡ç‰‡: {results['val_tiles']}")
        logger.info(f"ä¿ç•™å®Œæ•´æ ‡æ³¨: {results['kept_complete']}")
        logger.info(f"è·³è¿‡ä¸å®Œæ•´: {results['skipped_incomplete']}")
        logger.info("=" * 60)

    return results


if __name__ == "__main__":
    # æ¨¡å¼1: å¤„ç†å•ä¸ªæ–‡ä»¶
    # process_dataset("annotations/çº¢æœ¨.json")

    # æ¨¡å¼2: æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹
    # process_dataset("annotations")

    # æ¨¡å¼3: æ‰¹é‡å¤„ç† + åˆå¹¶æ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†
    process_dataset(
        input_source="annotations/çº¢æœ¨.json",
        merge_manual_datasets=True,
        manual_datasets_dir="datasets/manual_booth_annotations",
        final_output_dir="datasets/booth_final_merged",
        clean_temp=True,
        tile_size=640,
        overlap=200,
    )

