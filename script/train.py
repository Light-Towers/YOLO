from ultralytics import YOLO
import os
from pathlib import Path
import yaml
import re
import sys
import torch
from log_config import get_project_logger

# è·å–é¡¹ç›®logger
logger = get_project_logger('train')

# ==================== 1. è‡ªåŠ¨åŒ–é…ç½®æå– ====================
# [é…ç½®é¡¹] å¾…è®­ç»ƒçš„æ¨¡å‹åˆ—è¡¨
# è„šæœ¬ä¼šè‡ªåŠ¨æ ¹æ®æ¨¡å‹åç§°è¯†åˆ«ç‰ˆæœ¬ï¼ˆå¦‚ yolo11, yolov8, yolo26 ç­‰ï¼‰å¹¶å¯»æ‰¾é¢„è®­ç»ƒæƒé‡
models_to_train = [
    "yolo11m-obb.pt",
    # "yolov8s-obb.pt",
    # "yolo26n-obb.pt",
]

# [é…ç½®é¡¹] æ•°æ®é›†åç§° (datasets ç›®å½•ä¸‹çš„æ–‡ä»¶å¤¹å)
dataset_name = "fixed_tiled_dataset_1"

# [é…ç½®é¡¹] å®éªŒ/è½®æ¬¡åç§° (ç”¨äºåŒºåˆ†åŒä¸€æ¨¡å‹çš„ä¸åŒè®­ç»ƒé…ç½®)
exp_name = 'booth_obb_v1'

# [é…ç½®é¡¹] é¢„æµ‹å›¾åƒè·¯å¾„åˆ—è¡¨ (æ”¯æŒå•ä¸ªæˆ–å¤šä¸ªå›¾ç‰‡è·¯å¾„ï¼Œæˆ–å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„)
prediction_images = [
    "/home/aistudio/YOLO/images/2024å¹´å±•ä½å›¾.jpg",
    # "/home/aistudio/YOLO/images/ç¬¬åä¸€å±Šä¸–ç•ŒçŒªä¸šåšè§ˆä¼š.jpeg",
    # "/home/aistudio/YOLO/images/é•¿æ²™å›½é™…ä¼šå±•ä¸­å¿ƒ.jpg",
    # "/home/aistudio/YOLO/images/2020ç•œåšä¼š.png",
    # "/home/aistudio/YOLO/images/2025å¹´ç•œç‰§-å±•ä½åˆ†å¸ƒå›¾-1105-01.png"
]

# [åŠ¨æ€æ£€æµ‹] ç¡¬ä»¶èµ„æº
device = "0" if torch.cuda.is_available() else "cpu"
# åŠ¨æ€è®¡ç®—å·¥ä½œçº¿ç¨‹ï¼šå– CPU æ ¸å¿ƒæ•°çš„ä¸€åŠï¼Œæœ€å¤§ä¸è¶…è¿‡ 8
workers = min(8, (os.cpu_count() or 1) // 2)
# ========================================================

# 2. å®šä¹‰åŠ¨æ€åŸºç¡€è·¯å¾„
project_dir = Path(__file__).resolve().parent.parent
logger.info(f"Project root directory: {project_dir}")
logger.info(f"Using device: {device}, workers: {workers}")

# 3. è¾…åŠ©å‡½æ•°
def get_model_path(filename):
    """æ ¹æ®æ–‡ä»¶åè‡ªåŠ¨å®šä½é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„"""
    # åŒ¹é…ç‰ˆæœ¬å· (yolo11, yolov8, yolo26 ç­‰)ï¼Œä¸åŒºåˆ†å¤§å°å†™
    match = re.search(r'(yolo[a-z]*\d+)', filename.lower())
    version_dir = match.group(1) if match else ""
    return project_dir / 'models' / version_dir / filename

def update_dataset_path(yaml_path, new_base_path):
    """åŠ¨æ€æ›´æ–° dataset.yaml ä¸­çš„ path å­—æ®µ"""
    if not yaml_path.exists():
        logger.warning(f"{yaml_path} not found!")
        return

    with open(yaml_path, 'r', encoding='utf-8') as f:
        data = yaml.safe_load(f)
    
    # ç¡®ä¿ path æŒ‡å‘å½“å‰ç¯å¢ƒä¸‹çš„ç»å¯¹è·¯å¾„
    data['path'] = str(Path(new_base_path).resolve())
    
    with open(yaml_path, 'w', encoding='utf-8') as f:
        yaml.safe_dump(data, f, allow_unicode=True)
    logger.info(f"Updated dataset path in {yaml_path} to: {data['path']}")

def get_image_paths(image_sources):
    """
    ä»å›¾ç‰‡è·¯å¾„åˆ—è¡¨æˆ–æ–‡ä»¶å¤¹è·¯å¾„åˆ—è¡¨ä¸­è·å–æ‰€æœ‰å›¾ç‰‡è·¯å¾„
    """
    image_paths = []
    for source in image_sources:
        source_path = Path(source)
        if source_path.is_file():
            # å¦‚æœæ˜¯å•ä¸ªæ–‡ä»¶
            image_paths.append(str(source_path))
        elif source_path.is_dir():
            # å¦‚æœæ˜¯ç›®å½•ï¼Œè·å–ç›®å½•ä¸­æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
            extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff', '*.tif']
            for ext in extensions:
                image_paths.extend([str(p) for p in source_path.glob(ext)])
                image_paths.extend([str(p) for p in source_path.rglob(f"*.{ext.lstrip('*.')}")])
        else:
            logger.warning(f"Image source does not exist: {source}")
    
    return sorted(list(set(image_paths)))  # å»é‡å¹¶æ’åº

def run_predict_sahi_after_training(model_path, image_path):
    """
    åœ¨è®­ç»ƒå®Œæˆåé€šè¿‡è°ƒç”¨predict_sahiæ¨¡å—è¿è¡Œé¢„æµ‹
    """
    try:
        # å¯¼å…¥predict_sahiæ¨¡å—çš„start_predictå‡½æ•°
        from script.predict_sahi import start_predict
        
        logger.info(f"Starting prediction for image: {image_path}")
        
        # è°ƒç”¨predict_sahiæ¨¡å—çš„start_predictå‡½æ•°
        start_predict(model_path, image_path)
        
        logger.info("SAHI prediction completed successfully.")
    except ImportError:
        logger.error("Could not import predict_sahi module. Make sure predict_sahi.py is in the correct location.")
    except Exception as e:
        logger.error(f"Error occurred when calling predict_sahi module: {str(e)}")


# ==================== 4. ä¸»è®­ç»ƒå¾ªç¯ ====================

# å®šä½æ•°æ®é›†
dataset_root = project_dir / 'datasets' / dataset_name
dataset_yaml_path = dataset_root / 'dataset.yaml'

# æ›´æ–°æ•°æ®é›†è·¯å¾„é…ç½®
update_dataset_path(dataset_yaml_path, dataset_root)

# å­˜å‚¨æ‰€æœ‰è®­ç»ƒå®Œæˆçš„æ¨¡å‹è·¯å¾„å’Œåç§°
trained_models = []

for model_filename in models_to_train:
    logger.info(f"{'='*50}")
    logger.info(f"Starting training for model: {model_filename}")
    logger.info(f"{'='*50}")

    # è·å–é¢„è®­ç»ƒæ¨¡å‹ç»å¯¹è·¯å¾„
    yolo_model_path = get_model_path(model_filename)
    if not yolo_model_path.exists():
        logger.error(f"Pretrained model not found at {yolo_model_path}. Skipping...")
        continue

    # å®šä¹‰ç®€åŒ–çš„è¾“å‡ºè·¯å¾„: output/models/{model_name}/{exp_name}/
    # å»æ‰ .pt åç¼€ä½œä¸ºæ–‡ä»¶å¤¹å
    model_folder_name = Path(model_filename).stem
    train_save_dir = project_dir / 'output' / 'models' / model_folder_name

    # åŠ è½½æ¨¡å‹
    model = YOLO(str(yolo_model_path))

    # å¼€å§‹è®­ç»ƒ
    results = model.train(
        # æ•°æ®é›†é…ç½®æ–‡ä»¶
        data=str(dataset_yaml_path),
        
        epochs=300,                               # è®­ç»ƒè½®æ•°
        patience=50,                              # æ—©åœè€å¿ƒå€¼
        imgsz=640,                                # è¾“å…¥å›¾åƒå°ºå¯¸
        batch=0.9,                                # ã€3ç§æ–¹å¼ã€‘16ï¼šå›ºå®šæ–¹å¼ï¼›-1 è‡ªåŠ¨è®¡ç®—æœ€å¤§å¯ç”¨batchï¼› 0.8ï¼šæŒ‰gpuå†…å­˜åˆ†é…
        device=device,                            # è®­ç»ƒè®¾å¤‡ï¼ˆåŠ¨æ€æ£€æµ‹ï¼‰
        workers=workers,                          # å·¥ä½œçº¿ç¨‹æ•°ï¼ˆåŠ¨æ€æ£€æµ‹ï¼‰
        
        # ========== é¡¹ç›®ç›¸å…³å‚æ•° ==========
        project=str(train_save_dir),              # æŒ‡å®šæ¨¡å‹è®­ç»ƒè¾“å‡ºæ ¹ç›®å½•
        name=exp_name,                            # æŒ‡å®šå®éªŒåç§°
        save=True,                                # ä¿å­˜è®­ç»ƒç»“æœå’Œæ¨¡å‹
        save_period=-1,                           # ä»…åœ¨æœ€åä¿å­˜æ£€æŸ¥ç‚¹
        pretrained=True,                          # ä»é¢„è®­ç»ƒæ¨¡å‹å¼€å§‹è®­ç»ƒã€‚å¯ä»¥æ˜¯ä¸€ä¸ªå¸ƒå°”å€¼ï¼Œä¹Ÿå¯ä»¥æ˜¯åŠ è½½æƒé‡çš„ç‰¹å®šæ¨¡å‹çš„å­—ç¬¦ä¸²è·¯å¾„ã€‚å¢å¼ºè®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚
        
        # ========== è®­ç»ƒä¼˜åŒ–å‚æ•° ==========
        amp=True,                                 # å¼€å¯æ··åˆç²¾åº¦è®­ç»ƒï¼ŒæŸäº›æ˜¾å¡ä¸éœ€è¦
        cache=True,                               # å°†æ•°æ®é›†ç¼“å­˜åˆ°å†…å­˜ä¸­ ğŸš€
        compile=True,                             # å¼€å¯å†…æ ¸ç¼–è¯‘åŠ é€Ÿ
        
        # ========== å…³é”®ä¿®æ”¹3ï¼šè°ƒæ•´æ•°æ®å¢å¼ºç­–ç•¥ ==========
        # OBBä»»åŠ¡å¯¹æ—‹è½¬æ•æ„Ÿï¼Œéœ€è¦è°¨æ…è°ƒæ•´æ—‹è½¬å¢å¼º
        degrees=15.0,      # ã€å»ºè®®è°ƒä½ã€‘å±•ä½å›¾é€šå¸¸è§†è§’å›ºå®šï¼Œé¿å…è¿‡å¤§çš„æ—‹è½¬
        translate=0.1,     # å¹³ç§»å¢å¼º
        scale=0.5,         # ç¼©æ”¾å¢å¼º
        shear=0.0,         # ã€å»ºè®®å…³é—­ã€‘å‰ªåˆ‡å˜æ¢å¯èƒ½ç ´åæ—‹è½¬æ¡†çš„è§’åº¦ä¿¡æ¯
        perspective=0.001, # é€è§†å˜æ¢ï¼Œä¿æŒè¾ƒå°çš„å€¼
        flipud=0.0,        # ä¸Šä¸‹ç¿»è½¬ã€å»ºè®®å…³é—­ã€‘
        fliplr=0.5,        # å·¦å³ç¿»è½¬å¯ä¿ç•™
        
        # é©¬èµ›å…‹å¢å¼ºç›¸å…³
        mosaic=1.0,        # å¼€å¯é©¬èµ›å…‹å¢å¼º
        mixup=0.1,         # MixUpå¢å¼ºï¼Œä¸å®œè¿‡é«˜
        copy_paste=0.0,    # ã€å»ºè®®å…³é—­ã€‘å¤åˆ¶ç²˜è´´å¢å¼ºå¯èƒ½ä¸é€‚åˆOBB
        
        # ========== å…³é”®ä¿®æ”¹4ï¼šOBBç‰¹å®šå‚æ•° ==========
        # YOLO OBBä»»åŠ¡ä¼šè‡ªåŠ¨å¤„ç†æ—‹è½¬æ¡†ï¼Œä»¥ä¸‹æ˜¯å¯èƒ½éœ€è¦å…³æ³¨çš„å‚æ•°
        overlap_mask=False,  # ã€æ³¨æ„ã€‘OBBä»»åŠ¡ä¸éœ€è¦æ©ç é‡å ï¼Œåº”è¯¥è®¾ä¸ºFalse
        single_cls=True,     # å¦‚æœä½ çš„æ•°æ®é›†ä¸­åªæœ‰"å±•ä½"ä¸€ä¸ªç±»åˆ«ï¼Œè®¾ä¸ºTrue
        
        # ========== ä¼˜åŒ–å™¨ä¸å­¦ä¹ ç‡ ==========
        optimizer='auto',    # è‡ªåŠ¨é€‰æ‹©ä¼˜åŒ–å™¨ï¼š[SGD, Adam, AdamW, NAdam, RAdam, RMSProp]
        lr0=0.01,           # åˆå§‹å­¦ä¹ ç‡
        lrf=0.01,           # æœ€ç»ˆå­¦ä¹ ç‡ç³»æ•° (lr0 * lrf)
        momentum=0.937,     # åŠ¨é‡
        weight_decay=0.0005, # æƒé‡è¡°å‡
        warmup_epochs=3,    # å­¦ä¹ ç‡é¢„çƒ­è½®æ•°ï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒåˆæœŸ
        warmup_momentum=0.8, # é¢„çƒ­æœŸåŠ¨é‡
        warmup_bias_lr=0.1, # é¢„çƒ­æœŸåç½®å­¦ä¹ ç‡
        
        # ========== å…¶ä»–è°ƒæ•´ ==========
        dropout=0.0,        # OBBä»»åŠ¡é€šå¸¸ä¸éœ€è¦ï¼Œé˜²æ­¢å°æ•°æ®é›†è¿‡æ‹Ÿåˆ
        cos_lr=True,        # ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦ï¼Œå¯èƒ½å¸®åŠ©æ›´å¥½æ”¶æ•›
        # label_smoothing=0.0, # æ ‡ç­¾å¹³æ»‘ (å¼ƒç”¨)
        
        # ========== éªŒè¯ç›¸å…³å‚æ•° ==========
        val=True,           # åœ¨è®­ç»ƒæœŸé—´è¿›è¡ŒéªŒè¯
        plots=True,         # åœ¨è®­ç»ƒæœŸé—´ç”Ÿæˆå¹¶ä¿å­˜å›¾è¡¨
        resume=False,       # æ˜¯å¦ä»æœ€è¿‘çš„æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
        
        # ========== é’ˆå¯¹å¯†é›†å°ç›®æ ‡çš„è°ƒæ•´ ==========
        # å¦‚æœä½ çš„å±•ä½å¯†é›†ä¸”è¾ƒå°ï¼Œå¯ä»¥è€ƒè™‘ä»¥ä¸‹è°ƒæ•´
        # multi_scale=False,  # å¤šå°ºåº¦è®­ç»ƒï¼ˆä¼šå¢åŠ è®­ç»ƒæ—¶é—´ï¼‰
        # nbs=64,             # åä¹‰æ‰¹é‡å¤§å°
        
        # ========== è°ƒè¯•å‚æ•° ==========
        verbose=True,       # è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        deterministic=True, # ç¡®ä¿å¯é‡å¤æ€§
    )

    # è·å–è®­ç»ƒåçš„æœ€ä½³æ¨¡å‹è·¯å¾„
    best_model_path = train_save_dir / exp_name / 'weights' / 'best.pt'
    
    logger.info(f"Finished training for: {model_filename}")
    logger.info(f"Results saved in: {train_save_dir / exp_name}")

    # å°†è®­ç»ƒå®Œæˆçš„æ¨¡å‹è·¯å¾„æ·»åŠ åˆ°åˆ—è¡¨ä¸­
    trained_models.append((best_model_path, model_filename))

# æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œç»Ÿä¸€è¿›è¡Œé¢„æµ‹
logger.info("="*50)
logger.info("All training completed. Starting unified prediction for all models...")
logger.info("="*50)

# è·å–æ‰€æœ‰å›¾ç‰‡è·¯å¾„
all_image_paths = get_image_paths(prediction_images)
logger.info(f"Found {len(all_image_paths)} images to predict")

for model_path, model_filename in trained_models:
    logger.info(f"Running prediction for model: {model_filename}")
    for image_path in all_image_paths:
        run_predict_sahi_after_training(model_path, image_path)

logger.info("\næ‰€æœ‰è®­ç»ƒä»»åŠ¡å’Œé¢„æµ‹ä»»åŠ¡å·²å®Œæˆï¼")